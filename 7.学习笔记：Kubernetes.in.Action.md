# 学习笔记：Kubernetes.in.Action

## 1.kubernetes介绍

#### 1.1.kubernetes系统需求

>```shell
>1.单体应用到微服务（应用开大和部署在近些年的发展趋势）
>```
>
>>```shell
>>#单体应用：
>>	1.多个组件	2.紧密耦合	3.边界定义缺乏	4.系统复杂度提升	5.增长系统负荷
>>	6.垂直扩展（成本、瓶颈）	7.水平扩展（程序代码限制）
>>```
>>
>>```shell
>>#单体应用拆分为多个微服务
>>	1.独立部署	2.独立进程	3.接口通信（静态API）	4.协议通信	5.不局限编程语言
>>	6.独立开发	7.服务间改动依赖减小
>>```
>>
>>```shell
>>#微服务扩容
>>	1.针对单个服务	2.多个微服务	3.可以扩容的进行水平扩展	4.不可扩容进行垂直扩展	
>>```
>>
>>```shell
>>#部署微服务
>>	1.少许组件容易	2.组件增加、冗杂易错	3.扩机器和程序、调试代码和定位异常变得困难
>>	4.被分布式（如Zipkin）定位系统解决
>>```
>>
>>```shell
>>#环境需求差异
>>	1.独立开发组件导致需求环境的差异	2.多应用在同一台主机上运行可能会有依赖冲突
>>	3.同一主机上部署的组件越大，满足这些组件的需求就越难
>>```
>
>```shell
>2.为应用程序提供一个一致环境
>```
>
>>```shell
>>#减少仅在生产才会暴露的问题，最理想的做法是让程序在开发和生产阶段可以处在完全一样的环境下。
>>	1.操作系统	2.版本库	3.系统配置	4.网络环境	5.其他所有条件
>>```
>
>```shell
>3.迈向持续交付：Devops和无运维
>```
>
>>```shell
>>#让同一个团队参与应用的开发，部署，运维的整个生命周期，这意味着开发者、QA、和运维团队彼此之间的合作需要贯穿整个流程。这种实践被称为DevOps。
>>```
>>
>>```shell
>>#带来优点
>>	1.开发者更好理解用户以及运维团队问题	2.简化部署流程，理想状态开发人员可以自己部署上线
>>```
>>
>>```shell
>>#让开发者和系统管理员做他们最擅长的
>>	1.开发者热衷于创造新功能和提升用户体验
>>	2.运维团队热衷于系统安全、使用率、部署流程、硬件基础设置
>>```
>>
>>```shell
>>#kubernetes让我们实现某些想法
>>	1.抽象实际硬件，将自身暴露成一个平台，用于部署个运行环境
>>	2.允许开发者部署和配置应用程序，而不需要系统管理员的任何帮助
>>	3.系统管理员聚焦保持底层基础设施运转正常的同时，不需要关注实际运行在平台上的应用程序
>>```

#### 1.2.介绍容器技术

>```shell
>1.容器实现隔离技术
>```
>
>>```shell
>>#两个机制
>>	1.Linux的命名空间，它使每个进程只看到自己的系统视图（文件、进程、网络接口、主机名等）
>>	2.Linux控制组（cgroups），它限制了进程能够使用的资源量（CPU、内存、网络宽带等）
>>```
>>
>>```sh
>>#linux命名空间隔离进程
>>	1.默认情况，每个Linux系统最初仅有一个命名空间，所有系统资源属于这一个命名空间
>>	2.可以创建额外的命名空间，以及在它们之间创建资源
>>	3.对于一个进程，可以在其中一个命名空间运行它。进程就只能看到同一命名空间下的的资源
>>	4.存在多种类型的多个命名空间，一个进程不单单属于某一个命名空间，而属于每个类型的一个命名空间
>>		存在以下类型的命名空间：
>>		1.Mount(mnt)
>>		2.Process ID(pid)
>>		3.Network(net)
>>		4.Inter-process communication(ipd)
>>		5.UTS
>>		6.User ID(user)
>>	5.每种命名空间被用来隔离一组特定资源，UTS命名空间决定了运行在命名空间里的进程能够看到那些主	  机名和域名，通过分派两个不同的UTS命名空间给一对进程，能使它们看见不同的本地主机名，两个进	  程就好像在两个不同的机器上运行一样（至少就主机名而言是这样）
>>```
>>
>>```shell
>>#linux的cgroups限制进程可用资源
>>	1.linux的内核功能
>>	2.限制一个进程或者一组进程的资源使用
>>	3.不能超出被分配的资源量（CPU、内存、网络宽带等）
>>```
>
>```sh
>2.docker容器平台介绍
>```
>
>>```shell
>>#Docker的概念
>>	1.第一个使容器在不同机器之间移植的系统
>>	2.不关心部署的服务器是否安装了和你打包的应用使用相同的系统
>>		（用RHEL打包的应用在Debian或者其他的Fedora上运行，应用程序只认为它运行在RHEL上）
>>	3.docker是一个打包、分发和运行应用程序的平台
>>		1.允许将应用程序和所依赖的整个环境打包在一起（镜像）
>>		2.docker传输这个包到一个中央仓库（镜像仓库）
>>		3.分发到各个机器被执行（容器）
>>```
>>
>>```sh
>>#rkt——一个docker的替代方案
>>	1.docker本身并不提供进程隔离，由内核完成
>>	2.rkt是另外一个Linux容器引擎
>>	3.强调安全性，可构建性并遵从开放标准
>>	4.使用OCI开放容器计划容器镜像，甚至可以运行常规的docker镜像容器
>>	5.docker是kubernetes最初唯一支持的容器类型，最近也开始支持rkt
>>	6.kubernetes不是一个专为docker容器设计的容器编排系统，kubernetes的核心远不止编排容器
>>```

#### 1.3.Kubernetes介绍

>```shell
>1.初衷
>```
>
>>```sh
>>1.Google开发出来一个叫Borg的内部系统，后来开发一个新的叫Omega的系统，简化开发和管理
>>2.保守Borg和Omega秘密数十年后，2014年Google开放了Kubernetes
>>3.一个基于Borg、Omega及其他谷歌内部系统实践的开源系统
>>```
>
>```sh
>2.简单了解k8s
>```
>
>>```sh
>>1.一款软件系统
>>2.容易部署和管理容器化的系统
>>3.将底层基础抽象，Kubernetes使在数以千计的电脑节点上运行软件时就像所有这些节点使单个大节点一样
>>4.集群规模不会造成什么差异性，额外的集群节点只是代表一些额外可用来部署应用的资源
>>```
>>
>>​                                                                                                      ![image-20191218160513377](C:\Users\Owner\AppData\Roaming\Typora\typora-user-images\image-20191218160513377.png)
>
>```shell
>3.K8s的集群架构
>```
>
>>```sh
>>1.主节点：控制和管理整个集群系统的控制面板
>>2.工作节点：运行用户实际部署的应用
>>```
>>
>>![image-20191218160836837](C:\Users\Owner\AppData\Roaming\Typora\typora-user-images\image-20191218160836837.png)																																																				
>>
>>```sh
>>#控制面板（组件）
>>	1.Kubernetes API服务器，都要和它通信
>>	2.Schduler，它调度你的应用（为应用的每个部署组件分配一个工作节点）
>>	3.Controller Manager，它执行集群级别的功能，如辅助组件，持续跟踪工作节点，处理节点失败等
>>	4.Etecd,一个可靠的分布式数据存储，能够持久化存储集群配置
>>```
>>
>>```shell
>>#工作节点
>>	1.运行docker、rkt或者其他组件类型
>>	2.Kubelet，它与API服务器通信，并管理它所在的节点的容器
>>	3.Kubernetes Service Proxy (kube-proxy)，负责组件之间的负载均衡网络流量
>>```

#### 1.4.在Kubernetes中运行应用

>>```sh
>>1.描述信息怎样成为一个运行的容器
>>	#为了在K8s中运行应用，首先需要将应用打包进一个或多个容器镜像，再将镜像推送到镜像仓库，然后应用的描述发布到Kubernetes API服务器
>>	1.APP descriptor应用描述符列出容器并分组，组称为pod，同时确定需要运行每个pod的数量
>>	2.在想Kubernetes提交描述符后，它将把每个pod的指定副本数量调度到可用的工作节点上
>>	3.节点上的kebelet将告知docker从镜像仓库中拉取容器镜像并运行容器
>>```
>>
>>```sh
>>2.保持容器运行
>>	1.k8s不断地确认应用程序部署状态，始终与提供的描述想匹配，当进程崩溃或停止响应，自动重启
>>	2.如果整个工作节点死亡或者无法访问，k8s将故障节点运行的所有容器选择新节点，并运行
>>```
>>
>>![image-20191218174504982](C:\Users\Owner\AppData\Roaming\Typora\typora-user-images\image-20191218174504982.png)
>>
>>```sh
>>3.扩展副本数量
>>	1.kubernetes可以增加副本或者停止多余副本
>>	2.甚至可以把决定最佳副本数量交给kubernetes
>>	3.可以根据实时指标（如CPU消耗、内存消耗、每秒查询或公开的其他指标）自动调整副本数量
>>```
>>
>>```sh
>>4.命中移动容器
>>	1.运行节点失败，为其他容器腾地方等情况会导致容器迁移
>>	2.容器向运行在集群中的其他容器提供服务或者客户端提供服务，当容器在集群中频繁调度时
>>	3.如何正确使用该容器？容器被复制并分布整个集群时，客户端如何连接到提供服务的容器呢？
>>	4.Kubernetes将通过一个静态IP地址暴露所有容器，并将该地址暴露给集群中运行的所有应用程序
>>	5.环境变量完成或者通过良好的DNS查找服务IP
>>	6.kube-proxy将确保到服务的连接可跨提供服务的容器实现负载均衡
>>	7.服务的IP地址保持不变，客户端始终可以连接到它的容器
>>```

#### 1.5.使用Kubernetes的好处

>```sh
>1.简化应用程序部署
>2.更好地利用硬件
>3.健康检查和自修复
>4.自动扩容
>```

## 2.开始使用Kubernetes和Docker

#### 2.1.创建、运行及共享容器镜像

>>```sh
>>1.安装Docker并运行容器
>>http://docs.docker.corn/ engine/installation/  按照此文档安装即可
>>```
>>
>>```sh
>>2.运行Hello World容器
>>#busybox是一个单一可执行文件，包含多种标准UNIX命令工具：echo、ls、gzip等
>>docker run  busybox echo "Hello World"
>>```
>>
>>```shell
>>3.创建一个简单的Node.js应用
>>	1.构建一个简单的Node.js Web应用，并把它打包到容器镜像中
>>	2.这个应用会接受HTTP请求并响应应用的主机名
>>	3.Node.js代码清单
>>const http = require('http');
>>const os = require('OS');
>>console.log("Kubia server starting...");
>>var handler = function(request, response) {
>>  console.log("Received request from " + request.connection.remoteAddress);
>>  response.writeHead(200);
>>  response.end("You've hit " + os.hostname() + "\n");
>>};
>>var www = http.createServer(handler);
>>www.listen(8080);
>>	4.这里的8080端口启动一个HTTP服务器；服务器会以状态码200 OK和文字"You've hit <hostname>"
>>	  来响应每个请求。请求Handler 会把客户端的IP打印到标准输出，以便日后查看。
>>```
>>
>>```sh
>>4.为镜像创建Dockerfile
>>	FROM node:7
>>	ADD app.js /app.js
>>	ENTRYPOINT ["node", "app.js"]
>>#为什么非要用node这个基础镜像，因为运行的是Node.js应用，镜像需要包含可执行的node二进制文件来运行应用，也可以使用其他包含这个二进制文件的镜像，或者甚至可以使用Linux发型版的基础镜像，然后在构建时候暗转Node.js。但是由于node镜像是专门用来运行Node.js应用的，并且包含了应用所需的一切，所以把它当做基础镜像。
>>```
>>
>>```sh
>>5.构建容器镜像
>>docker build -t kubia .
>>```
>>
>>![image-20191218185840833](C:\Users\Owner\AppData\Roaming\Typora\typora-user-images\image-20191218185840833.png)
>>
>>```sh
>>#镜像是如何构建的
>>	1.执行命令
>>	2.Docker客户端将整个目录文件Docker守护进程并在那里进行
>>	3.Docker的客户端和守护进程不要求在同一台机子上
>>	4.构建的文件上传到守护进程，可能时间比较久
>>	5.Docker首次会从公开的镜像仓库（Docker Hub）拉取基础镜像，除非已经拉去过
>>```
>>
>>```sh
>>6.运行镜像
>>docker run --name kubia-container -p 8080:8080 -d kubia
>>
>>#尝试访问  curl localhost:8080
>>#详细信息  docker inspect kubia-container  
>>		#docker  会打印出包含容器底层信息的长JSON
>>```
>>
>>```sh
>>7.向镜像仓库推送镜像
>>docker login
>>docker tag kubia  docker.io/bingyuantin/kubia
>>docker push docker.io/bingyuantin/kubia
>>```

#### 2.2.配置Kubernetes集群

>```sh
>1.单节点Kubernetes集群Minikube
>```
>
>>```sh
>>1.安装Minikube
>>	1.按照http://github.com/kubernetes/minikube提示来安装
>>	2.curl -Lo minikube https://storage.googleapis.com/minikube/releases
>>	/v0.23.0/minikube-darwin-amd64 && chmod +x minikube && sudo mv minikube
>>	/usr/local/bin/
>>	3.linux系统将其中的darwin替换成linux
>>	4.minikube在vm中通过virtualbox或者kvm运行kubernetes，启动前安装vm
>>2.启动一个minikube虚拟机
>>	1.minikube start
>>	2.启动集群需要花费超过一分钟时间，命令完成前不要中断
>>3.安装kubernetes客户端（kubectl）
>>	1.curl -LO https://storage.googleapis.com/kubernetes-release/release
>>	  /$(curl -s https://storage.googleapis.com/kubernetes-release/release
>>	  /stable.txt)/bin/darwin/amd64/kubectl
>>	  && chmod +x kubectl
>>	  && sudo mv kubectl /usr/local/bin/
>>	2.linux系统用linux替换url中的darwin
>>4.验证集群是否正常
>>	1.kubectl cluster-info
>>	2.显示了各种Kubernetes组件的URL , 包括API服务器和Web控制台
>>```
>
>```sh
>2.使用Google Kubernetes Engine托管Kubernetes 集群
>```
>
>>```sh
>>	#想探索一个完善的多节点Kubernetes集群，可以使用托管的Google Kubernetes Engine (GKE)集群。无需手动配置所有的集群节点和网络，不会出现配置错误，不工作或者部分工作的集群。
>>1.配置一个Google项目并且下载必须的客户端二进制
>>	#设置GKE环境，参考读https://cloud.google.com/containerengine/docs/before-begin
>>	1.注册google账户
>>	2.在Google Cloud Platform控制台中创建一个项目
>>	3.开启账单。这会需要你的信用卡信息，但是谷歌提供了为期12个月的免费试用，结束不自动续费
>>	4.开启Kubernetes Engine API
>>	5.下载安装Google Cloud SDK（这包含gcloud命令行工具，需要创建一个Kubernetes集群）
>>	6.使用gcloud components install kubectl 安装kubectl 命令行工具
>>2.创建一个三节点Kubernetes集群
>>	1.gcloud container clusters create kubia --num-nodes 3
>>		--machine-type fl-micro
>>3.获取集群概览
>>	1.通过kubectl命令行客户端向运行在节点上的Kubernetes API服务器发出REST请求与集群交互
>>	2.列出节点查看集群是否在运行
>>		kubectl get nodes
>>	3.查看对象更多信息
>>		kubectl describe node x-x-x
>>```
>
>```sh
>3.为kubectl配置别名和命令行补齐
>```
>
>>```sh
>>1.创建别名
>>	1.vim  /root/.bashrc
>>	2.alias  k=kubectl
>>	#注意：如果你已经在用gcloud配置集群，就已经有可执行文件K了
>>2.为kubectl配置补全
>>	1.安装bashcompletion的包来启用bash中的tab命令补全
>>	2.运行：source <(kubectl completion bash)
>>	3.运行：source <(kubectl completion bash I sed s/kubectl/k/g)
>>	#似乎出现点问题
>>```

#### 2.3.在Kubernetes上运行第一个应用

>```sh
>1.部署Node.js应用
>```
>
>>```sh
>>1.最简单命令kubectl run ，该命令创建有必要的组件而无需JSON或YAML文件
>>2.运行之前推送到docker hub 的镜像
>>3.kubectl run kubia --image=bingyauntin/kubia --port=8080 --generator=run/vl
>>#--image=bingyuantin/kubia 显示指定运行的容器
>>#--port=8080  告诉kubernetes应用正在监听8080端口
>>#--generator  让kubernetes创建一个ReplicationController，而不是Deployment
>>```
>
>```sh
>2.介绍pod
>```
>
>>```sh
>>1.多个容器共存，这组容器就是pod
>>2.运行在同一个节点上
>>3.运行在同一个linux的命令空间
>>4.每个pod拥有自己的IP，主机名，进程
>>5.pod分布在不同的节点上
>>6.kubectl get pods -o wide #列出pod
>>```
>
>```sh
>3.幕后发生的事情
>```
>
>>```sh
>>1.构建镜像并将其推送到Docker Hub上 
>>#因为本地构建的镜像只能在本地使用，需要访问在工作节点的Docker守护进程
>>2.运行kubectl命令，向kubernetes API服务器发送REST HTTP请求，在集群中创建RC对象
>>3.RC创建一个新的pod，调度器将其调度到一个工作节点上
>>4.kubelet看到pod被调度到节点上，就告知Docker从镜像中心拉取指定镜像，本地无则下载镜像创建容器
>>```
>>
>>​	![image-20191219131557615](C:\Users\Owner\AppData\Roaming\Typora\typora-user-images\image-20191219131557615.png)

#### 2.4.访问WEB应用

>```sh
>1.创建一个服务对象
>```
>
>>```sh
>>1.通过对象服务公开pod内部地址，需要创建Loadbalancer类型服务，常规的ClusterIP服务只能内部访问
>>2.创建LoadBalancer类型的服务，将创建一个外部的负载均衡，可以通过负载均衡的公共IP访问pod
>>3.创建服务，告知Kubernetes对外暴露之前创建的rc
>>  kubectl expose rc kubia --type=LoadBalancer --name kubia-http 
>>```
>
>```sh
>2.列出服务
>```
>
>>```sh
>>1.expose命令的输出中提到一个名为kubia-http的服务，该服务是类似于pod和node的对象
>>2.通过kubectl get services命令查看新创建的服务对象
>>3.显示的EXTERNAL-IP为外部ip
>>4.使用外部ip向pod服务发送请求：curl http://ip:8080
>>5.使用minikube的时候，可以运行minikube service kubia-http 获取服务ip和端口号
>>```

#### 2.5.系统的逻辑部分

>```sh
>1.rc、pod、service是如何组合在一起
>```
>
>>```
>>1.kubectl run 命令创建rc，rc用来创建pod实例
>>2.外部访问rc，需要kubernetes将rc管理的所有pod由一个服务对外暴露
>>```
>>
>>​	![image-20191219135851431](C:\Users\Owner\AppData\Roaming\Typora\typora-user-images\image-20191219135851431.png)
>
>```sh
>2.pod和它的容器
>```
>
>>```sh
>>1.在你的系统中最重要的组件是pod
>>2.包含一个容器，通常可以包含任意数量的容器
>>3.容器内部是Node.js进程，该进程绑定到8080端口，等待HTTP请求
>>4.pod有自己独立的IP和主机名
>>```
>
>```sh
>3.ReplicationController的角色
>```
>
>>```sh
>>1.下一个组件kubia ReplicationController 确保始终存在一个运行中的pod实例
>>2.ReplicationController用于辅助pod并让它保持运行
>>3.如果pod消失，ReplicationController将创建一个新的pod来替换消失的pod
>>```
>
>```sh
>4.为甚需要服务
>```
>
>>```sh
>>1.上面提到的替换新的pod与旧的pod的ip是不同的
>>2.解决不断变化的pod IP地址问题，在一个固定的ip和port对上对外暴露对个pod
>>```

#### 2.6.水平拉伸

>```sh
>1.增加期望副本数
>	kubectl scale rc kubia --replicas=3
>```
>
>```sh
>2.查看应用运行在哪个节点
>	kubectl get pods -o wide 
>```
>
>```
>3.查看pod细节
>	kubectl	describe pod x-x-x
>```

#### 2.7.kubernetes dashboard

>```sh
>1.访问GKE集群的dashboard
>```
>
>>```sh
>>1.如果使用的是Google Kubernetes Engine，通过kubectl cluster-info命令找到dashboard的URL
>>2.kubectl cluster-info | grep dashboard
>>3.在浏览器中打开这个URL
>>4.gcloud container	clusters describe kubia | grep -E "(username|password):"
>>5.要打开minikube的kubernetes集群的dashboard
>>	minikube dashboard #不需要输入任何凭证
>>```

## 3.pod：运行于Kubernetes中的容器

#### 3.1.介绍pod

>>```sh
>>1.为什么需要pod
>>	1.不能将多个进程聚集在一个单独的容器中，我们需要另一种更高级的结构将容器绑定在一起，并将它们作为一个单元进行管理，这就是pod的根本原理
>>	2.同一pod中容器之间的部分隔离，容器共享相同的linux命名空间
>>	3.相同的network和UTS命名空间下运行，有相同的主机名和网络接口
>>	4.相同的IPC命名空间下运行，因此可以通过IPC进行通信
>>2.容器如何共享相同的IP和端口空间
>>	1.运行于相同的network命名空间，贡献个相同的IP和端口空间
>>	2.一个pod中的容器运行的多个进程需要注意不能绑定相同的端口号，否则会导致端口冲突
>>	3.一个pod中的容器有相同的loopback，可以通过localhost与同一pod中的其他容器进行通信
>>3.平坦的pod间网络
>>	1.kubernetes集群中的所有pod都在同一个共享网络地址空间中
>>	2.每个pod都可以通过其他pod的ip地址来实现相互访问
>>	3.pod之间没有NAT（网络地址转换）网关
>>	4.两个pod之间发送网络数据时，都会把对方实际ip看做数据包中的源ip
>>	5.通过专门网络实现pod之间相互访问，专门网络通常由额外的软件基于真实的链路实现
>>```

#### 3.2.以YAML和JSON描述文件创建pod

>```sh
>1.检查现有pod的YAML描述文件
>```
>
>>```sh
>>1.kubectl get po kubia-zxzij -o yaml
>>2.kubectl get po kubia-zxzij -o json
>>```
>
>```sh
>2.pod定义的主要部分
>```
>
>>```sh
>>1.apiVersion:	Kubernetes API版本
>>2.kind:	YAML描述的资源类型
>>3.metadata：	包括名称、命名空间、标签和关于该容器的其他信息
>>4.spec：pod内容的实际说明，例如：pod的容器、卷和其他数据
>>5.status：包含运行中pod当前信息，例如：pod所处条件、每个容器的描述和状态，内部ip及其他信息
>>```
>
>```sh
>3.为pod创建一个简单的YAML文件
>```
>
>>```sh
>>apiVersion:	v1	# kubectl api-versions 命名可以获取版本
>>kind:	pod
>>metadata:
>>    name:kubia-manual
>>spec:
>>    containers:
>>    -   image:	bingyuantin/kubia
>>        name:	kubia
>>        ports:
>>        -   containerPort:	8080
>>            protocol: TCP
>>#1.pod定义中的端口完全是展示性的，忽略它们对于客户端是否可以通过端口连接到pod不会带来任何影响。
>>#2.如果容器通过绑定到地址0.0.0.0的端口接收连接，那么即使端口未明确列出在pod.spec中，其他pod依旧能够连接到该端口
>>```
>
>```shell
>4.使用kubectl create 来创建pod
>```
>
>>```sh
>>1.使用kubectl create 命令从YAML文件创建pod
>>	kubectl create -f kubia-manual.yaml
>>	# kubectl create -f命令用于从YAML或JSON文件创建任何资源（不只是pod)
>>2.得到运行中pod 的完整定义
>>	kubectl get po kubia-manual -o yaml
>>	kubectl get po kubia-manual -o json
>>3.在pod 列表中查看新创建的pod
>>	kubectl get pods
>>	#1.pending状态：
>>	kubectl describe ...来查看
>>	资源不足: 你可能耗尽了集群上所有的CPU和内存，你需要删除pods，调整资源请求，或者增加节点
>>	使用了hostPort: 如果绑定一个pod到hostPort，那么能创建的pod个数就有限了，在大多数情况，					   hostPort是非必须的，应该采用服务来暴露pod，如果确实需要使用hostPort，					   那么能创建pod的数量就是节点的个数
>>	#2.waiting状态：
>>	kubectl describe ...来查看
>>	镜像名字是否正确
>>	镜像仓库是否有这个镜像
>>	用docker pull <image>拉取镜像试下
>>	#3.crashing或者unhealthy
>>	看log：kubectl logs ${POD_NAME} ${CONTAINER_NAME}
>>	如果容器是crashed，用下面命令查看log：
>>		kubectl logs  --previous ${POD_NAME} ${CONTAINER_NAME}
>>		kubectl exec ${POD_NAME} -c ${CONTAINER_NAME} -- ${CMD} ${ARG1} ...${ARGN}
>>	   #kubectl exec cassandra -- cat /var/log/cassandra/system.log
>>	#4.pod处于running状态，但是没有正常工作
>>	一般是创建pod的描述文件应该存在某种错误，并且这个错误在创建时候被忽略掉
>>	删掉之前的pod，在带上参数--validate重新创建一个：
>>		kubectl create --validate -f xxxx.yaml
>>		注意观察输出信息
>>	如果没有异常信息，接下来验证从apiserver获取到的pod是否与期望一致
>>		kubectl get pod xxx -o yaml  >  xxx.yaml
>>		对标xxx.yaml和创建时的yaml文件
>>4.向pod发送请求
>>	1.将本地网络端口转发到pod 中的端口
>>		不通过service的情况下与某个特定的pod通信，kubernetes允许我们配置端口转发到pod
>>		kubectl port-forward kubia-manual 8888:8080
>>	2.通过端口转发连接到pod
>>		curl localhost:8888
>>		用端口转发是一种测试特定pod的有效方法
>>```

#### 3.3.使用标签组织pod

>```sh
>1.介绍标签
>```
>
>>```sh
>>1.标签是可以附加到资源的任意键值对
>>2.通过给pod添加标签，可以得到一个更组织化的系统
>>```
>
>```shell
>2.创建pod时指定标签
>```
>
>>```sh
>>apiVersion: vl
>>kind: Pod
>>metadata:
>>  name: kubia-manual-v2
>>  labels:
>>    creation method: manual   #两个标签被
>>    env: prod                 #添加到pod上
>>spec:
>>  containers:
>>  - image: luksa/kubia
>>    name: kubia
>>    ports:
>>    - containerPort： 8080
>>      protocol: TCP
>>```
>>
>>```shell
>>kubectl get po --show-labels #查看pod时显示标签
>>```
>>
>>```sh
>>kubectl get po -L creation_method,env #列出pod，并展示标签
>>```
>
>```sh
>3.修改现有标签
>```
>
>>```sh
>>1.添加标签
>>  kubectl label po kubia-manual	creation_method=manual
>>2.修改现有标签
>>  kubectl label po kubia-manual-v2	env=debug --overwrite
>>3.再次查看pod的标签
>>  kubectl get po -L creation_method,env
>>```
>
>```sh
>4.通过标签选择器列出pod子集
>```
>
>>```sh
>>1.列出creation_method=manual的pod
>>  kubectl get po -l creation_method=manual
>>2.列出包含env标签的所有pod
>>  kubectl get po -l env
>>3.列出没有env标签的所有pod
>>  kubectl get po -l '!env'
>>4.pod与标签选择器的匹配
>>  creation_method!=manual选择带有creation_method标签，并且值不等于manual的pod
>>  env in (prod,devel)选择带有env标签且值为prod或devel的pod
>>  env notin (prod,devel)选择带有env标签，但其值不是prod或devel的pod
>>5.在标签选择器中使用多个条件
>>  kubect get po -l app=pc,rel=beta
>>```
>
>```shell
>5.使用标签和选择器来约束pod调度
>```
>
>> ```shell
>> 1.前言：
>> 	1.之前创建的pod都是随机调度到工作节点上，这恰恰是k8s正确的调度方式
>> 	2.对于pod而言，它获得所请求的确切数量的计算资源(CPU、内存等)及其他pod的可访问性，完全不受        该pod   所调度到的节点的影响，所以通常没有任何需要指定k8s把pod调度到哪里
>> 	3.如果相对一个pod拥有发言权，那就不应该直接指定一个节点，而应该用某种方式描述对节点的需求
>> ```
>>
>> ```sh
>> 2.使用标签分类工作节点
>> 	1.kubectl get nodes  #列出所有节点
>> 	2.kubectl label	node x-x-x GPU=ture
>> 	3.kubectl get nodes -l GPU=ture
>> 	  kubectl get nodes -l GPU
>> ```
>>
>> ```shell
>> 3.将pod调度到特定节点
>> 	1.vim  /etc/k8s_yaml/kubia-gpu.yaml
>> 	apiVersion: vl
>> 	kind: Pod
>> 	metadata:
>> 	  name: kubia-gpu
>> 	spec:
>> 	  nodeSelector:
>> 	    gpu: "true"
>> 	  containers:
>> 	  - image: bingyuantin/kubia
>> 		name: kubia
>> 	2.kubectl create -f kubia-gpu.yaml
>> 	3.只是在spec部分添加了一个nodeSelector字段，当创建pod时，调度器只在包含标签gpu=true节点  	  中选择
>> ```
>>
>> ```shell
>> 4.调度到一个特定节点
>> 	1.每个节点都有一个确定的唯一标签，其中键为kubernetes.io/hostname,值为该节点实际主机名
>> 	2.可以将pod调度到某个确定节点，但如果节点处于离线状态，通过此hostname标签将nodeSelector设	   置为特定节点，可能会导致pod不可调度
>> 	3.绝不应考虑单个节点，而是应该通过标签选择器考虑符合特定标准的逻辑节点组。
>> ```
>
>```sh
>6.注释pod
>```
>
>>```sh
>>1.查找对象的注释
>>	1.注释也是键值对，与标签非常相似
>>	2.不能对对象分组，通过标签选择器选择对象时，就不存在注解选择器这样的东西
>>	3.注解可以容纳更多东西
>>	4.kubectl get po kubia-xxx -o yaml
>>```
>>
>>```sh
>>2.添加和修改注解
>>	1.通过kubectl annotate命令添加注解
>>	  kubec七1 annotate pod kubia-manual mycompany.com/someannotation="foo bar"
>>	2.通过kubectl describe命令来查看刚才的注解
>>```
>
>```sh
>7.使用命名空间对资源进行分组
>```
>
>>```sh
>>1.应用命名空间
>>	1.发现其他命名空间
>>	  kubectl get ns
>>	  NAME          STATUS    AGE
>>	  default       Active    6d
>>	  kube-system   Active    6d
>>	2.列出只属于特定命名空间的pod
>>	  kubectl get po --namespace kube-system  #可以使用-n来代替--namespace
>>	  No resources found.
>>	3.如果有多个用户或者用户组在使用k8s，并且他们都各自管理自己独特的资源集合，那么就应该分别使       用各自的命名空间
>>	4.命名空间为资源名称提供了一个作用域
>>```
>>
>>```sh
>>2.创建一个命名空间
>>	1.以YAML文件创建命名空间
>>	  vim /etc/k8s_yaml/custom_namespace.yaml
>>	  apiVersion: v1
>>	  kind: namespace
>>	  metadata: 
>>	    name: custom_namespace
>>	2.使用kubectl将文件提交到Kubernetes API服务器
>>	  kubectl create -f /etc/k8s_yaml/custom_namespace.yaml
>>	3.使用kubectl命令创建空间名
>>  	  kubectl create namespace custom_namespace
>>```
>>
>>```sh
>>3.管理其他命名空间中的对象
>>	1.命名创建资源时指定命名空间
>>	  kubectl create /etc/k8s_yaml/kubia-manual.yaml -n custom-namespace
>>	2.想要快速切换到不同的命名空间，可以设置一下别名：
>>	  alias kcd=' kubectl config set context $ (kubectl config current context)
>>                 --namespace '
>>       使用： kcd some-namespace在命名空间之间进行切换
>>```
>>
>>```sh
>>4.删除pod
>>	1.正常删除
>>	  kubectl delete pod xxx  yyy   zzz
>>	2.利用标签删除
>>	  kubectl delete pod -l creation_method=manual
>>	3.删除命名空间来删除整个命名空间的pod
>>	  kubectl delete ns custom-namespace
>>	4.删除命名空间所有pod，但是保留命名空间
>>	  kubectl delete po --all
>>	  # 如果有在命令行kubectl run创建的rc，则删除po后会再生成po，所以需要删除创建的rc
>>	5.删除命名空间几乎所有资源
>>	  kubectl delete all --all
>>	  # all并不是真正删除所有每个资源，一些资源会被保留下来，并且需要被明确指定删除
>>	  # kubectl delete all --all 也会删除名为kubernetes的Service，但几分钟后会自建
>>```

## 4.副本机制和其他控制器：部署托管的pod

#### 4.1.保持pod健康

>```sh
>#为什么需要存活探针
>	默认情况下Kubernetes只是检查Pod容器是否正常运行，但容器正常运行并不一定代表应用健康，在以下两种情况下Kubernetes将不会重启容器：
>	1.访问Web服务器时显示500内部错误
>	#该报错可能是系统超载，也可能是资源死锁，不过此时httpd进程依旧运行，重启容器可能是最直接有效的办法。
>	2.具有内存泄漏的Java应用程序将开始抛出OutOfMemoryErrors
>	#此时JVM进程会一直运行，Kubernetes也不会重启容器，但此时对应用来讲是异常的。
>
>#此时可以考虑从外部检查应用程序的运行状况：
>Kubemetes可以通过存活探针(liveness probe)检查容器是否还在运行；
>通过就绪探针(readiness probe)保证只有准备好了请求的Pod才能接收客户端请求。
>```
>
>>```sh
>>1.介绍存活探针liveness Probe
>>	1.k8s可以通过liveness Probe检查容器是否还在运行
>>	2.可以为pod中的每个容器单独指定liveness Probe
>>	3.如果探测失败，k8s定期执行探针，并重新启动容器
>>	4.k8s有一下三种探测机制：
>>		1.HTTP GET探针对容器的IP地址执行HTTP GET请求，如果探测器收到响应，响应代码不代表错              误，认为探测成功；错误响应状态码或者没有响应，那么探测就被认为是失败的，将重启容器
>>		2.TCP套接字探针尝试与容器指定端口建立TCP连接，建立成功则探测成功，否则重启容器
>>		3.Exec探针在容器内执行任意命令，并检查命令的推出状态码，0位成功，其他均为失败
>>```
>>
>>```shell
>>2.创建基于HTTP GET的存活探针
>>	1.将存活探针添加到pod：kubia-liveness-probe.yaml
>>	  apiVersion: v1
>>	  kind: pod
>>	  metadata: 
>>	    name: kubia-liveness
>>	  spec:
>>	    containers:
>>	    - images: bingyuantin/kubia-unhealthy
>>	      name: kubia
>>	      livenessProbe:
>>	        httpGet:
>>	          path: / 
>>	          port: 8080
>>	2.该pod描述文件定义了一个httpGet存活探针，告诉k8s定期在端口8080路径上执行HTTP GET请求，	   以确定该容器是否健康。这些请求在容器运行后立即开始。
>>	3.立即创建pod，pod容器将别重启，kubectl get pod kubia-liveness
>>```
>>
>>```shell
>>3.使用存活探针
>>	1.获取崩溃容器的应用日志
>>	  kubectl logs ${CONTAINER_NAME} --previous  #查看上一个容器日志
>>	2.了解为什么要重启容器
>>	  kubectl describe po kubia-liveness
>>	  Events:
>>		.. Killing container with id docker://95246981:pod "kubia-liveness
>>		container "kubia" is unhealthy, it will be killed and re-created
>>	  #可以看出是因为container unhealthy  不健康导致的
>>	  #退出代码137，有特殊的含义--表示进程由外部信号终止。137是128+x，x是9，SIGKILL的信号编码	   #意味着这个进程被强行终止，此时会创建一个全新的容器，而不是重启原来的容器
>>```
>>
>>```shell
>>4.配置存活探针的附加属性
>>	1.kubectl describe 还显示关于存活探针的附加信息：
>>	  Liveness: http-gethttp://:8080/ delay=Os timeout=ls period=lOs #success=l
>>                 #failure=3
>>	2.delay=Os部分显示在容器启动后立即开始探测
>>	  timeout仅设置为1秒，因此容器必须在1秒内进行响应， 不然这次探测记作失败
>>	  每10秒探测一次容器(period=lOs), 并在探测连续三次失败(#failure=3)后重启容器
>>	3.务必记得设置一个初始延迟未说明应用程序的启动时间，如果没有设置一个初始延迟时间，探针将会在       启动时立即探测容器，因为应用程序还没准备好开始接收请求，通常会导致探测失败，如果失败次数达       到阈值，在应用程序能够真确响应请求之前，容器就会重启
>>	4.通常看到退出码为137、143（SIGTERM），大多是因为未能适当设置initialDelaySeconds
>>	5.设置适当的initialDelaySeconds
>>	  apiVersion: v1
>>	  kind: pod
>>	  metadata: 
>>	    name: kubia-liveness
>>	  spec:
>>	    containers:
>>	    - images: bingyuantin/kubia-unhealthy
>>	      name: kubia
>>	      livenessProbe:
>>	        httpGet:
>>	          path: / 
>>	          port: 8080
>>	        initialDelaySeconds: 15
>>```
>>
>>```sh
>>5.创建有效的存活探针
>>	1.对于生产中运行的pod，一定要定义一个存活探针，否则kubernetes不知道应用程序是否活着，因为	  只要进程还在运行，kubernetes会认为容器是健康的
>>	2.存活探针应该检查什么
>>		1.简易的存活探针仅仅检查服务器是是否响应，为了更好的进行存活检测，需要将探针配置为特定  			的URL路径（例如 /health），并让应用从内部对内部运行的所有重要组件执行状态检查，以确			 保它们没有终止或者停止响应
>>		#请确保/health HTTP瑞点不需要认证， 否则探剧会一直失败， 导致你的容器无限重
>>		2.一定要检查应用程序的内部，而没有任何外部因素影响。例如，当服务器不能连接到后端数据库		  时，前端web服务器的存活探测不应该返回失败。如果底层问题在数据库，重启web服务器是没有			 作用的，存活探针依然失败，将会反复重启容器直到数据库恢复
>>	3.保持探针轻量
>>		#如果你在容器中运行Java应用程序，请确保使用HTTP GET存活探针，而不是启动全新JVM以荻取 		   存活信息的Exec探针。任何基于JVM或类似的应用程序也是如此，它们启动过程需要大量计算资源
>>	4.无须在探针中实现重试循环
>>		1.探针的失败阙值是可配置的
>>		2.通常在容器被终止之前探针必须失败多次
>>		3.即使你将失败阙值设置为1，Kubemetes为了确认一次探测的失败，会尝试若干次
>>		4.在探针中自己实现重试循环是浪费精力
>>	5.容器停止由该节点的kubelet执行重启，节点崩溃由主节点的Kubernetes Control Plane组件才会       参与
>>```

#### 4.2.了解ReplicationController

>>```sh
>>1.ReplicationController的操作
>>	1.直接被创建的pod，节点异常退出，RC不会负责在其他节点上重建pod
>>	  由RC托管的pod，节点异常退出，RC会负责在其他接点上重建pod
>>	2.ReplicationController的三部分
>>		1.label selector，用于确定RC作用域中有那些种类pod
>>		2.replica count，指定应运行的pod数量
>>		3.pod template，由于创建新的pod
>>		#只有副本数目的变化会影响现有的pod
>>	3.更改控制器的标签选择器或pod 模板的效果
>>		1.更改标签选择器和pod 模板对现有pod 没有影响
>>		2.更改标签选择器会使现有的pod脱离ReplicationController的范围
>>		3.pod模板更改仅会影响后面新建的pod
>>	4.使用ReplicationController 的好处
>>		1.确保一个pod (或多个pod 副本）持续运行， 方法是在现有pod 丢失时启动一个新pod
>>		2.集群节点发生故障时， 它将为故障节点上运行的所有pod (即受ReplicationController 控		  制的节点上的那些pod) 创建替代副本
>>		3.它能轻松实现pod的水平伸缩手动和自动都可以
>>```
>>
>>```sh
>>2.创建一个ReplicationController
>>	1.创建一个kubia-rc.yaml的YAML文件
>>	vim  /etc/k8s_yaml/kubia-rc.yaml
>>	apiVersion: v1	
>>	kind: ReplicationController
>>	metadata:
>>	  name: kubia  #ReplicationController的名字
>>	spec:
>>	  replicas: 3  #pod实例的数目
>>	  selector:
>>	    app: kubia  #pod选择器决定了RC的操作对象
>>	  template:     #创建新pod所用的pod模板
>>	    metadata:
>>	      labels:
>>	        app: kubia
>>	    spec:
>>	      containers:
>>	      - name: kubia
>>	        image： bingyuantin/kubia
>>	        port:
>>	        - containerPort: 8080
>>	2.上传到API服务器时，Kubernetes会创建一个名为kubia的新ReplicationController,它确保标签	  选择器app=kubia的pod实例始终是三个，没有足够pod时候，会根据提供的pod模板新建pod
>>	3.模板中的pod标签显然必须和ReplicationController的标签选择器匹配，否则控制器将无休止地创	   建新的容器
>>	4.定义ReplicationController时不要指定pod选择器，让Kubernetes从pod模板中提取它。这样		  YAML更简短
>>	5.创建：kubectl create -f kubia-rc.yaml
>>```
>>
>>```sh
>>3.使用ReplicationController
>>	1.没有任何pod有app=kubia的标签，ReplicationController会根据pod模板启动三个新的pod
>>	2.kubectl get po
>>	3.kubectl delete po  xxx  # 查看rc对已删除的pod的响应
>>	  重新列出pod会显示四个，删除的pod会终止，并且已创建一个新的pod
>>```
>>
>>```sh
>>4.获取有关ReplicationController 的信息
>>	1.kubectl get rc
>>	2.kubectl describe rc  kubia
>>```

#### 4.3.将pod 移入或移出ReplicationController 的作用域

>```sh
>#由ReplicationController创建的pod并不是绑定到ReplicationController。ReplicationController管理与标签选择器匹配的pod。通过更改pod的标签，可以将它从ReplicationController的作用域中添加或删除，甚至可以从一个ReplicationController移动到另外一个
>```
>
>>```sh
>>1.给Replication Controller 管理的pod 加标签
>>kubectl label pod xxx type=special
>>2.列出pod
>>kubectl get pods  --show-labels
>>#发现还是以前的三个pod，没有改变
>>3.更改己托管的pod 的标签
>>kubectl label pod  xxx app=foo  --overwirte
>>#--overwrite 参数是必要的，否则kubectl 将只打印出警告，并不会更改标签。这样是为了防止你想要添加新标签时无意中更改现有标签的值
>>4.再次列出pod
>>kubectl get pods -L app
>>```
>>
>>```sh
>>5.修改pod模板
>>	1.kubectl edit rc xxx
>>	2.修改ReplicationController的YAML配置，找到pod模板部分向元数据添加一个新的标签
>>	3.rc会打印：replicationcontroller "kubia" edited
>>	4.列出pod及标签，确认为发生变化
>>	5.删除pod，并创建，在查看发现新标签
>>	6.此方法可以用来升级pod
>>	7.kubectl edit 可以使用不同的编辑器
>>		vim ~/.bashrc
>>		export KUBE_EDITOR="/usr/bin/nano"   #使用nano编辑器
>>		#如果未设置，将使用默认编辑器
>>6.水平放缩pod
>>	1.可以在yaml文件中的spec.replicas键所对应的值
>>	2.可以在命令行修改replicas所对应的值
>>		kubectl	scale	rc  xxx   --replicas=10
>>	3.声明的方式修改spec.relicas的值
>>		kubectl edit rc xxx
>>		然后将其中的spec.replicas的值修改即可
>>7.删除一个ReplicationController
>>	1.使用kubectl delete 删除ReplicationController时，可以增加--cascade=false保持pod运行
>>		kubectl	delete	rc  xxx --cascade=false
>>	2.pod独立了，不再被管理
>>	3.可以使用适当的标签管理器创建新的ReplicationController，再次将它们管理起来
>>```

#### 4.4.使用ReplicaSet而不是ReplicationController

>>```sh
>>1.ReplicaSet是ReplicationController的升级，两者几乎完全相同
>>2.一般不会创建它们，而是创建Deployment资源时自动创建它们
>>```
>>
>>```sh
>>3.比较rs和rc
>>	1.rs的pod选择器的表达能力更强
>>		#允许匹配缺少某个标签的pod，或包含特定标签的pod
>>	2.单个rc无法将pod与标签env=production和env=devel同时匹配，只能匹配带有env=production或	   env=devel的标签的pod，但是rs可以匹配两组pod并将它们视为一个大组
>>	3.无论rc的的值如何，rc都无法仅基于标签名的存在来匹配pod，而rs可以。例如rs可以匹配所有包含		  标签名为env的标签的pod，无论rs的实际值是什么（可以理解为env=*）
>>```
>>
>>```sh
>>4.定义rs
>>	1.创建rs来接管之前被rc删除的但依然在运行的无主pod
>>	2.vim /etc/k8s_yaml/kubia-replicaset.yaml
>>		apiVersion: apps/v1beta2       #不是v1版本API的一部分，属于apps API组v1beta2版本
>>		kind: ReplicaSet               #和低版本中的extensions/v1beta1
>>		metadata:
>>		  name: kubia
>>		spec:
>>		  replicas: 3
>>		  selector:
>>		    matchLabels:               #选择了更简单的matchLabels选择器，类似于rc的选择器
>>		      app: kubia
>>		  template:	                   #该模板与rc中的相同
>>		    metadata:
>>		      labels:
>>		        app: kubia
>>		    spec:
>>		      containers:
>>		      - name: kubia
>>		        image: bingyuantin/kubia
>>	3.#唯一的区别在选择器中。
>>	  #不必在selector属性中直接列出pod需要的标签，而是在selector.matchLabels下制定，这是rs        中定义标签选择器的更简单（也更具表达力）的方式。
>>```
>>
>>```sh
>>5.使用rs更富表达力的标签选择器
>>	1.使用更强大的matchExpressions属性来重写选择器
>>		selector:
>>		  matchExpressions:            #此选择器要求该pod包含名为"app"的标签
>>		    - key: app
>>		      operator: In
>>		      values:
>>		        - kubia                #标签的值必须是"kubia"
>>	2.每个表达式包含一个key，一个operator(运算符)，并且还可能有一个values的列表(取决运算符)
>>	3.四个有效的运算符：
>>		In：Label的值必须与其中一个指定的values匹配
>>		NotIn：Label的值与任何指定的values不匹配
>>		Exists：pod必须包含一个指定名称的标签(值不重要)，使用此运算符时，不应指定values字段
>>		DoesNotExist：pod不得包含有指定名称的标签。values属性不得指定
>>```
>>
>>```sh
>>6.k8s的apiVersion改用哪个，简单分析
>>	1.kubernetes官方并未对apiVersion做过多的解释，而且因为k8s本身版本也在快速迭代，有些资源在       低版本的beta阶段，到了高版本就成了stable
>>	2.Deployment
>>		1.1.6版本之前apiVersion: extensions/v1beta1
>>		2.1.6版本到1.9版本之间: apps/v1beta1
>>		3.1.9版本之后: apps/v1
>>	3.各种apiVersion的含义
>>		1.alpha：该软件可能包含错误，启用一个功能可能会导致bug
>>				随时可能丢弃对该功能的支持，恕不另行通知
>>		2.beta：软件经过很好的测试，启动功能被认为是安全的
>>			    默认情况下功能是开启的
>>			    细节可能会改变，但功能在后续版本不会被删除
>>		3.stable：该版本的命名方式：vX，这里的X是一个整数
>>				 稳定版本放心使用
>>				 将出现在后续发布的软件版本中
>>		4.v1
>>			kubernetes API的稳定版本，包含多个核心对象：pod、service等
>>		5.apps/v1beta2
>>			在kubernetes1.8版本中，新增加了apps/v1beta2的概念，app/v1beta1同理
>>			DaemonSet，Deployment，ReplicaSet 和 StatefulSet的当时版本迁入apps/v1beta2
>>			兼容原有的extensions/v1beta1
>>		6.apps/v1
>>			在kubernetes1.9版本中引入apps/v1
>>			deployment等资源从extensions/v1beta1,apps/v1beta1和apps/v1beta2迁入apps/v1
>>			apps/v1代表：包含一些通用的应用层的api组合，如Deployments, RollingUpdates,                             and ReplicaSets
>>		7.batch/v1
>>			代表job相关的api组合
>>			在kubernetes1.8版本中，新增了batch/v1beta1，CronJob 已经迁移到batch/v1beta1
>>		8.autoscaling/v1
>>			代表自动扩容的api组合，kubernetes1.8版本中引入
>>			这个组合中后续的alpha和beta版本将支持基于memory使用量、其他监控指标进行扩容
>>		9.extensions/v1beat1
>>			Deployment等资源在1.6版本时放在这个版本中，后迁入到apps/v1beta2，再到apps/v1
>>		10.certificates.k8s.io/v1beta1
>>			安全认证的相关组合
>>		11.authentication.k8s.io/v1
>>			资源鉴权相关的api组合
>>		12.查看API版本
>>			kubectl api-versions
>>```
>>
>>```sh
>>7.使用DaemonSet在每个节点上运行一个pod
>>	1.rc和rs都用于在kubernetes上运行部署特定的数量的pod
>>	2.希望pod集群在每个节点上运行时，并且每个节点都需要正好一个运行的pod实例
>>	3.这些情况包括：pod执行系统级别的与基础结构相关的操作。
>>	  例如：在每个节点运行日志收集器和资源监控器
>>	  	    kubernetes自己的kube-proxy进程，需要运行在所有节点上才能使服务工作
>>	4.DaemonSet并没有期望的副本数的概念，只保证每个节点有个一DaemonSet管理的pod
>>	5.DaemonSet 从配置的pod 模板创建pod
>>	6.nodeSelector 属性可以指定在某些特定的node上部署pod
>>	7.Daemon Set 管理的pod 则可以完全绕过调度器
>>	8.创建一个DaemonSet.yaml定义文件
>>		apiVersion: apps/v1beat2 #低版本可以用extensions/v1beta1代替
>>		kind: DaemonSet
>>		metadata:
>>		  name: ssd-monitor
>>		spec:
>>		  selector:
>>		    matchLabels:
>>		      app: ssd-monitor
>>		  template:
>>		    metadata:
>>		      labels:
>>		        app: ssd-monitor
>>		    spec:
>>		      nodeSelector:   #pod模板包含一个节点选择器，会选择有disk=ssd标签的节点
>>		        disk: ssd
>>		      containers:
>>		      - name: main
>>		        image: luksa/ssd-monitor
>>	9.kubectl get ds  #来查看
>>```

#### 4.5.运行执行单个任务的pod

>>```sh
>>1.介绍job资源	
>>	1.ReplicationController 、ReplicaSet和DaemonSet 会持续运行任务，永远达不到完成态    
>>	2.Kubernetes通过Job资源提供了对此的支持，它允许你运行一种pod, 该pod在内部进程成功结束时，       不重启容器，一旦任务完成，pod就被认为处于完成状态。
>>	3.发生节点故障时，job管理的pod将安装ReplicaSet的pod方式，重新安排到其他节点
>>	4.job对于临时任务很有用，关键是任务要以正确方式结束
>>```
>>
>>```sh
>>2.定义Job资源
>>	1.Job的yaml定义
>>	vim /etc/k8s_yaml/exorter.yaml
>>	apiVersion: batch/v1
>>	kind: Job
>>	metadata:
>>	  name: batch-job
>>	spec:
>>	  template:
>>	    metadata:
>>	      labels:
>>	        app: batch-job
>>	    spec:
>>	      restartPolicy: OnFailure #Job不能使用Always的重启策略
>>	      containers: 
>>	      - name: main
>>	        image: luksa/batch-job #该镜像调用一个运行120秒的进程
>>	2.在一个pod定义中，可以指定在容器中运行的进程结束时，kubernetes会做什么，这是通过pod配置的
>>	  restartPolicy完成的，默认为Always，但是Job的pod不能使用默认策略，因为它不是要无限期运       行下去，因此要明确地重启策略设置成OnFailure或Never，防止容器在弯沉过任务时候重启
>>```
>>
>>```sh
>>3.看Job运行一个pod
>>	1.kubectl get jobs
>>	2.kubectl get pod
>>	3.进程结束后，不再显示完成的pod，需要使用--show-all（或-a）
>>	  kubectl get pod --show-all
>>	4.完成后pod未被删除的原因是允许你查询其日志
>>	  kubectl logs ${POD_NAME}
>>```
>>
>>```sh
>>4.在Job中运行多个pod实例
>>	1.通过在Job配置中设置completions和parallelism属性来完成
>>	2.vim /etc/k8s_yaml/multi-completion-batch-job.yaml
>>	   apiVersion: batch/v1
>>	   kind: Job
>>	   metadata:
>>	     name: multi-completion-batch-job
>>	   spec:
>>	     completions: 5  #此作业顺序运行五个pod
>>	     template:
>>	3.Job将会一个接一个地运行五个pod，一个结束再运行下一个
>>	4.vim /etc/k8s_yaml/multi-completion-parallel-batch-job.yaml
>>		apiVersion: batch/v1
>>		kind: Job
>>		metadata:
>>		  name: multi-completion-parallel-batch-job
>>		sepc:
>>		  completions: 5
>>		  parallelism: 2  #最多两个pod并行
>>		  template:
>>	5.限制Job pod的完成任务时间
>>		1.pod卡住并且根本无法完成，怎么办？
>>		2.在pod配置中设置activeDeadlineSeconds属性
>>		3.如果pod运行时间超过此时间，系统将尝试终止pod，并将Job标记为失败
>>	6.Job pod失败前可以重试次数，未明确指定，则为6次
>>		spec.backoffLimit  字段来限制
>>```
>>
>>```sh
>>5.安排Job定期运行或在将来运行一次
>>	1.Kubernetes 中的cron 任务通过创建CronJob 资源进行配置
>>	2.创建一个CronJob
>>		apiVersion: batch/v1beta1   #旧版本为batch/v1
>>		kind: CronJob
>>		metadata:
>>		  name: batch-job-every-fifteen-minutes
>>		spec:
>>		  schedule: "0,15,30,45 * * * *" #这项工作应该在每天0、15、30、45分钟运行 
>>		  jobTemplate:    #此CronJob创建Job资源会用到的模板
>>		    spec:
>>		      template:
>>		        metadata:
>>		          labels:
>>		            app: periodic-batch-job
>>		        spec:
>>		          restartPolicy: OnFailure
>>		          containers:
>>		          - name: main
>>		            image: luksa/batch-job
>>	3.五个星号遵循：分 时 日 月 周
>>```
>>
>>```sh
>>6.了解计划任务的运行方式
>>	1.在计划的时间内，CronJob资源会创建Job资源，然后Job创建pod
>>	2.如果需要任务开始不能落后于预定的时间过多，可以通过制定CronJob规范中的           
>>	  startingDeadlineSeconds字段来指定截止日期，如下所示：
>>	  apiVersion: batch/v1beta1
>>	  kind: CronJob
>>	  metadata: 
>>	    name: batch-job-every-fifteen-minutes
>>	  spec:
>>	    schedule: "0,15,30,45 * * * *"
>>	    startingDeadlineSeconds: 15 #pod最迟必须在预定时间后15秒开始运行
>>```
>

## 5.服务：让客户端发现pod并与之通信

#### 5.1介绍服务

>​	![image-20191221173516288](C:\Users\Owner\AppData\Roaming\Typora\typora-user-images\image-20191221173516288.png)
>
>>```sh
>>5.1.1创建服务
>>	1.通过kubectl expose创建服务
>>		kubectl expose rc kubia --type=LoadBalancer --name kubia-http
>>		#2.4章节有介绍
>>	2.通过YAML文件传递到Kubernetes API服务器来创建服务
>>	vim  /etc/k8s_yaml/kubia-svc.yaml
>>	apiVersion: v1
>>	kind: Service
>>	metadata:
>>	  name: kubia
>>	spec:
>>	  ports:
>>	  - port: 80			#该服务的可用端口
>>	    targetPort: 8080	 #将连接转发到的容器端口
>>	  selector:
>>	    app: kubia			#具有app=kubia标签的所有pod属于该服务
>>	3.kubectl create -f  /etc/k8s_yaml/kubia-svc.yaml
>>	4.kubectl get svc  #列表会显示分配给服务的ip地址，但只是集群地址，只能内部访问
>>	5.从内部集群测试服务
>>		1.创建一个pod，它将请求发送到服务器集群ip，可通过查看pod日志检查服务响应
>>		2.远程ssh登录到kubernetes节点上，然后使用curl命令
>>		3.kubect exec命令在一个已经存在的pod中执行curl命令
>>		  kubectl exec kubia-xxx -- curl -s http://$ip:80
>>	6.配置服务会话亲和性
>>	#每次连接的应该在不同的pod上
>>	#想要客户端请求每次都只想同一个pod，可以设置sessionAfinity属性为ClientIP(None是默认值)
>>	apiVersion：v1
>>	kind： service
>>	spec:
>>	  sessionAfinity: CLientIP   #来自同一clientIP请求发送至同一个pod上
>>	  ...
>>	7.同一个服务暴露多个端口
>>		1.例如你的pod监听两个端口，http监听8080端口，https监听8443端口，可以使用一个service            从80和443分别转发到8080和8443
>>		2.使用一个IP集群，一个服务就可以将多个端口全部暴露出来
>>		3.在创建有多个端口的服务的时候，必须给每个端口指定名字
>>		4.	apiVersion: v1
>>			kind: Service
>>			metadata:
>>			  name:kubia
>>			spec:
>>			  ports:
>>			  - name: http
>>			    port: 80
>>			    targetPort: 8080
>>			  - name: https
>>			    port: 443
>>			    targetPort: 8443
>>			  selector:
>>			    app: kubia
>>		5.使用命名的端口
>>			1.pod的定义中指定port名称
>>			apiVersion: v1
>>			kind: Pod
>>			metadata：
>>			  name: kubia
>>			spec:
>>			  containers:
>>			  - name: http
>>			    containerPort: 8080
>>			  - name: https
>>			    containerPort: 8443
>>			2.在服务中应用命名的pod
>>			apiVersion: v1
>>			kind: Service
>>			metadata:
>>			  name: kubia
>>			spec:
>>			  ports:
>>			  - name: http
>>			    port: 80
>>			    targetPort: http
>>			  - name: https
>>			    port: 443
>>			    targetPort: https
>>		6.采用命名端口的方式，最大的好处就是即使更换端口号也无需更改服务spec
>>```
>>
>>```sh
>>5.1.2服务发现（客户端如何发现服务的ip和port）
>>	1.通过环境变量发现服务
>>		1.创建的service早于客户端pod，pod上的进程可以根据环境变量获得服务的ip和port
>>		2.创建的service晚于客户端pod，这个服务的环境变量并没有设置
>>			1.查看服务的环境变量之前，先删除所有的pod，使得rc新建所有pod
>>			kubectl delete po --all
>>			2.相关环境变量KUBIA_SERVICE_HOST和KUBIA_SERVICE_PORT代表kubia服务的ip和port
>>			3.当前端pod需要后端数据库服务pod时，通过backend-database将后端pod暴露出来
>>			  后端pod通过环境变量BACKEND_DATABASE_SERVICE_HOST和        	     
>>			  BACKEND_DATABASE_SERVICE_PORT去获得IP地址和port
>>			4.服务名称中的横杠被转化成下划线，服务名称用作环境变量名称前缀时，所有字母都是大写
>>	2.通过DNS发现服务
>>		1.pod上的进程DNS查询都会被kubernetes自身的DNS服务器响应，该服务器知道系统中运行的所            有服务
>>		2.pod是否使用内部的DNS服务器是根据pod中的spec的dnsPolicy属性决定的
>>		3.每个服务从内部DNS服务器中获得一个DNS条目，客户端的pod在知道服务名称的情况下可以通过            全限定域名（FQDN）来访问，而不是诉诸于环境变量
>>	3.通过FQDN连接服务
>>		1.前端pod可以通过打开以下FQDN的连接来访问后端数据库服务
>>		  backend-database.default.svc.cluster.local
>>		backend-database对应服务名，default表示服务在其中定义的名称空间，svc.cluster.local          是在所有集群本地服务名称中使用的可配置集群域后缀
>>		2.如果前端pod和数据库pod在同一个命名空间下，可以省略svc.cluster.local后缀，甚至命名          空间，因此可以使用backend-database来指代服务			
>>```

#### 5.2.连接外部服务

>>```sh
>>5.2.1介绍服务endpoint
>>	1.kubectl  describe svc  xxx  会显示其中的endpoint
>>	2.服务不是pod直接相连的，而是通过endpoint资源
>>	3.endpoint资源就是暴露一服务的ip和端口的列表
>>	4.查看endpoint资源
>>	kubectl get endpoint  xxx
>>```
>>
>>```sh
>>5.2.2手动配置endpoint
>>	1.创建一个yaml文件,定义一个服务，接受端口80上的传入连接，并没有为服务定义一个pod选择器
>>	vim /etc/k8s_yaml/external-service.yaml
>>	apiVersion: v1
>>	kind: Service
>>	metadata: 
>>	  name: external-service
>>	spec:
>>	  ports:
>>	  - port: 80
>>	2.为没有选择器的服务创建endpoint资源
>>	vim /etc/k8s_yaml/external-service-endpoints.yaml
>>	apiVersion: v1
>>	kind: Endpoints
>>	metadata:
>>	  name: external-service
>>	subsets:
>>	  - addresses:
>>	    - ip：11.11.11.11
>>	    - ip：22.22.22.22
>>	    ports：
>>	    - port: 80
>>```
>>
>>![image-20191223130941255](C:\Users\Owner\AppData\Roaming\Typora\typora-user-images\image-20191223130941255.png)
>>
>>```sh
>>5.2.3为外部服务创建别名
>>	1.通过endpoint来代替公开外部服务，或者通过其完全限定域名访问外部服务
>>	2.创建ExternelName类型的服务
>>	vim /etc/k8s_yaml/external-service-externalname.yaml
>>	apiVersion: v1
>>	kind: Service
>>	metadata:
>>	  name： external-service
>>	spec:
>>	  type: ExternalName
>>	  externalName: someapi.somecompany.com #设想someapi.somecompany.com上有公共可用API
>>	  ports:
>>	  - port: 80
>>	3.服务创建完后，pod可以通过external-service.default.svc.cluster.local域名连接外部服务
>>	4.ExternalName服务仅在DNS级别实施，为服务创建了简单的CNAME DNS记录
>>	5.CNAME记录指向完全限定的域名而不是数字IP地址
>>```

#### 5.3将服务暴露给外部客户端

>```sh
>#几种方式可以在外部访问服务：
>	#1.将服务的类型设置成NodePort，每个集群节点都将在节点上打开一个端口，接收到的流量重定向到基础服务
>	#2.将服务的类型设置成LoadBalance，NodePort类型的一种扩展，服务可以通过一个专用的负载均衡器来访问，         由kubernetes中正在运行的云基础设施提供的。负载均衡器将流量重定向到跨所有节点的节点端口。客户端         通过负载均衡器的IP连接到服务
>	#3.创建一个Ingress资源，这是一个完全不同的机制，通过一个ip地址公开多个服务。它运行在http网络七层          上，可以提供比4层更多的服务
>```
>
>![image-20191223132544423](C:\Users\Owner\AppData\Roaming\Typora\typora-user-images\image-20191223132544423.png)
>
>>```sh
>>5.3.1创建NodePort类型服务
>>apiVersion: v1
>>kind: Service 
>>metadata：
>>  name: kubia-nodeport
>>spec:
>>  type: NodePort
>>  ports:
>>  - port: 80
>>    targetPort: 8080
>>    nodePort: 30123 #如果未设置，则会选择随机端口
>>  selector:
>>    app: kubia
>>```
>>
>>![image-20191223134522425](C:\Users\Owner\AppData\Roaming\Typora\typora-user-images\image-20191223134522425.png)
>>
>>```sh
>>5.3.2通过负载均衡器LoadBalancer将服务暴露出来
>>	1.云提供商上运行的kubernetes集群设置服务类型为LoadBalancer而不是NodePort
>>	2.负载均衡器有自己的独一无二的公开访问的IP地址，将所有连接重定向到服务，通过负载均衡IP访问
>>	3.创建Load Balancer类型服务
>>	vim /etc/k8s_yaml/kubia-svc-loadbalancer.yaml
>>	apiVersion: v1
>>	kind: Service
>>	betadata:
>>	  name:kubia-loadbalancer
>>	spec:
>>	  type: LoadBalancer
>>	  ports:
>>	  - port: 80
>>	    targetPort: 8080
>>	  selector:
>>	    app: kubia
>>```
>>
>>​	![image-20191223140940816](C:\Users\Owner\AppData\Roaming\Typora\typora-user-images\image-20191223140940816.png)
>>
>>```sh
>>5.3.3了解外部连接特性
>>	1.防止不必要的网络跳转
>>	#随机选择的pod并不一定在同一节点上运行，需要网路额外跳转才能到达pod
>>	通过spec部分中设置externalTrafficPolicy字段完成节点与pod的同一
>>	spec:
>>	  externalTrafficPolicy: Local
>>	  ...
>>	2.集群内部客户端连接服务时候，支持服务的pod可以获取客户端的IP地址
>>	  通过节点端口接收连接时，由于对数据包进行了源网络地址转换(SNAT),数据包源IP发生更改，后端        pod无法看到源ip，对于一些需要了解客户端ip的应用程序来说，访问日志无法显示浏览器的IP
>>```

#### 5.4通过Ingress暴露服务

>>```sh
>>5.4.1
>>	1.Ingress只需要一个公网IP就可以为许多服务提供访问
>>	2.客户端向Ingress发送HTTP服务时，Ingress会根据请求的主机名和路径来决定请求转发到的服务
>>```
>>
>>![image-20191223144053893](C:\Users\Owner\AppData\Roaming\Typora\typora-user-images\image-20191223144053893.png)
>>
>>```sh
>>    3.Ingress在网络栈(HTTP)的应用层操作，能提供一些服务不能实现的功能，基于cookie的会话亲和性
>>    4.只有Ingress控制器在集群中运行，Ingress资源才能正常工作
>>    5.查看所有命名空间的资源，确认集群中正在运行Ingress控制器
>>	kubectl get pods --all-namespaces
>>	6.创建Ingress资源
>>	apiVersion: extensions/v1beta1
>>	kind: Ingress
>>	metadata:
>>	  name: kubia
>>	spec:
>>	  rules:
>>	  - host: kubia.example.com #Ingress将域名kubia.example.com映射到你的服务
>>	    http:
>>	      paths:
>>	      - path: /
>>	        backend:
>>	          serviceName: kubia-nodeport #将所有的请求发送到kubia-nodeport服务的80端口
>>	          servicePort: 80
>>	7.定义一个单一规则的Ingress，确保Ingress控制器收到的所有请求主机kubia.example.com的HTTP       请求，将被发送到端口80的kubia-nodeport服务
>>```
>>
>>```sh
>>5.4.2通过Ingress访问服务
>>	1.http://kubia.example.com访问服务，需要确保域名解析为Ingress控制器的IP
>>	2.获取Ingress的IP地址 kubectl get ingresses
>>	3.通过配置DNS服务器将kubia.example.com解析为此ip地址
>>	4.Ingress工作原理
>>	 最终通过与Ingress中定义的rules和path确定要访问的服务，再通过与该服务关联的Endpoint对象查      看pod IP
>>```
>>
>>![image-20191223151723593](C:\Users\Owner\AppData\Roaming\Typora\typora-user-images\image-20191223151723593.png)
>>
>>```SH
>>5.4.3通过相同的Ingress暴露多个服务
>>	1.将不同的服务映射到相同主机的不同路径下
>>	...
>>	- host: kubia.example.com
>>	  http: 
>>	    paths:
>>	    - path: /kubia
>>	      backend:
>>	        serviceName: kubia
>>	        servicePort: 80
>>	    - path: /foo
>>	      backend:
>>	        serviceName: bar
>>	        servicePort: 80
>>	2.根据url路径将会把请求发送至不同服务，因此客户端可以通过一个IP地址访问两种不同的服务
>>	3.将不同服务映射到不同的主机上
>>	...
>>	spec:
>>	  rules:
>>	  - host: foo.example.com
>>	    http:
>>	      paths:
>>	      - path: /
>>	        backend:
>>	          serviceName: foo
>>	          servicePort: 80
>>	  - host: bar.example.com
>>	    http:
>>	      paths:
>>	      - path: /
>>	        backend:
>>	          serviceName: bar
>>	          servicePort: 80
>>	4.根据请求中的HOST头，控制器收到请求将被转发到foo服务或者bar服务，需要在DNS中将域名指向          Ingress控制器的IP
>>```
>>
>>```sh
>>5.4.4配置Ingress的TLS传输（https如何转发）
>>	1.为Ingress创建TLS认证
>>		1.需要将证书和私钥附加到Ingress
>>		2.这两个必需资源存储在称为Secret的Kubernetes资源中
>>		3.在Ingress manifest中引用它
>>		4.创建私钥和证书
>>		openssl genrsa -out tls.key 2048
>>		openssl req -new -x509 -key tls.key -out tls.cert -days 360 -subj
>>		/CN=kubia.example.com
>>		#生成CA私钥.key-->生成CA证书请求.csr-->自签名得到根证书.crt（CA给自已颁发的证书）
>>		# Generate CA private key 
>>         openssl genrsa -out ca.key 2048 
>>         # Generate CSR 
>>         openssl req -new -key ca.key -out ca.csr
>>         # Generate Self Signed certificate（CA 根证书）
>>         openssl x509 -req -days 365 -in ca.csr -signkey ca.key -out ca.crt
>>		5.创建secret资源存储证书和私钥
>>		kubectl create secret tls tls-secret --cert=tls.cert --key=tls.key
>>	2.通过CertificateSigningRequest资源签署证书
>>		1.可以不通过自己签署证书，而是通过CSR资源来签署
>>		2.用户或应用可以创建一个常规证书请求，将其放入CSR中，然后由人工或自动化程序批准请求
>>		kubectl certificate approve <name of CSR>
>>		3.然后可以从CSR的status.certificate字段中检索签名的证书
>>		4.证书签署者组件须在集群中运行，否则CertificateSigningRequest以及批准拒绝不起作用
>>	3.更新Ingress对象
>>		1.私钥和证书存储在名为tls-secret的secret中，现在可以更新Ingress对象
>>		2.vim /etc/k8s_yaml/kubia-ingress-tls.yaml
>>		apiVersion: extensions/v1beta1
>>		kind: Ingress
>>		metadata:
>>		  name: kubia
>>		spce:
>>		  tls:
>>		  - hosts:
>>		    - kubia.example.com
>>		    secretName: tls-secret
>>		  rules:
>>		  - host: kubia.example.com
>>		    http:
>>		      paths:
>>		      - path: /
>>		        backend:
>>		          serviceName: kubia-nodeport
>>		          servicePort: 80
>>		3.通过调用kubectl apply -f kubia-ingress-tls.yaml使用文件中指定的内容来更新                   Ingress资源，而不是删除并从新文件重新创建的方式
>>```

#### 5.5pod就绪后发出信号

###### 5.5.1介绍就绪探针

>```sh
>1.一个新建的pod，如果pod的标签和service的选择器相匹配，那么pod就将作为service的后端，service就第一时间选择到它，把请求转发给pod。通常一个pod启动是需要时间的，如果pod还未准备好，这时就会造成请求失败。给pod加一个就绪探针，当检测到pod就绪后才允许service请求转给pod
>2.就绪探针有三种类型，主要来确定容器是否就绪
>	1.Exec探针
>	2.HTTP GET探针
>	3.TCP socket探针
>3.了解就绪探针操作
>	1.利用Endpoints来实现Readiness Probe的效果
>	2.当pod还未就绪时候，将pod的IP:Port在中删除
>	3.pod就绪后再加入到Endpoints中
>	4.最终就绪探针确保客户端只与正常的pod交互，并且永远不会知道系统存在问题
>```
>
>![image-20191224102421807](C:\Users\Owner\AppData\Roaming\Typora\typora-user-images\image-20191224102421807.png)

###### 5.5.2向pod添加就绪探针

>```sh
>1.可以通过kubectl edit命令来向已存在的ReplicationController中的pod添加探针
>2.kubectl edit rc kubia
>3.添加在spec.template.spec.containers下的第一个容器
>4.spec:
>    ...
>    template:
>      ...
>      spec:
>        containers:
>        - name: kubia
>          images: bingyuantin/kubia
>          readinessProbe:
>            exec:
>              command:
>              - ls             #pod中的每个容器都会有一个就绪探针
>              - /var/ready
>        ...
>5.就绪探针定期在容器执行ls  /var/ready命令，如果文件存在，则ls命令返回退出码0，否则返回非零退出码
>6.创建这样一个探针主要是因为可以通过创建或删除有问题的文件来出发结果
>7.此时kubectl get po  发现pod并未准备好，因为没有/var/ready文件
>8.通过创建/var/ready文件使其中一个文件的就绪探针返回成功，模拟就绪探针成功
>	kubectl exec kubia-xxx   -- touch /var/ready
>9.probe中精确和详细的配置
>	#initialDelaySeconds: 10 容器启动后第一次执行探针需要等待多少秒
>	#periodSeconds: 10 执行探测的频率，默认10秒，最小1秒
>	#timeoutSeconds: 1 探测超时时间，默认1秒，最小1秒
>	#successThreshold: 1 连续探测1次成功表示成功
>	#failureThreshold: 3 连续探测3次失败表示失败
>```

#### 5.6使用headless服务来发现独立的pod

###### 5.6.1创建headless服务

>```sh
>1.将service中的clusterIP字段设置成None会使service成为headless服务
>2.apiVersion: v1
>  kind: Service
>  betadata:
>    name: kubia-headless
>  spec:
>    clusterIP: None     #这使得服务成为headless的
>    ports:
>    - port: 80
>      targetPort: 8080
>    selector:
>      app: kubia
>3.使用kubectl create 创建服务后，kubectl get 和kubectl describe来查看服务，没有集群ip，并且它的后端包含与pod选择器匹配的pod，因为有一部分pod尚未就绪
>```

###### 5.6.2通过DNS发现pod

>```sh
>1.准备好pod，执行DNS查找，需要nslooku或dig二进制文件
>2.不通过yaml文件运行一个DNS的pod
>   kubectl run dnsutils --image=tutum/dnsutils --generator=run-pod/v1 --command -- sleep        infinity
>   #--generator=run-pod/v1选项，让kubectl直接创建pod，而不需要通过rc之类的资源来创建
>3.理解headless服务的DNS A记录解析
>  使用新创建的pod执行DNS查找：
>  kubectl exec dnsutils nslookup kubia-headless
>  							   （#service）
>4.headless服务返回的是准备就绪的pod的IP
>  非headless服务返回的是服务的集群IP
>5.对用户来说并无不同，headless服务可以通过连接DNS名称来连接到pod，由于DNS直接返回了pod的IP，客户端直接   连接到该pod，而不是通过服务代理
>6.headless服务依然提供跨pod的负载均衡，只是通过DNS轮询机制，而不是代理服务kube-proxy
>```

5.6.3发现所有的pod，包括未就绪的pod

>```sh
>1.不用通过查询kubernetes API服务器，可以使用DNS查找机制来查找那些未准备好的pod
>2.要告诉kubernetes无论pod的准备状态如何，希望将所有pod添加到服务中，需在服务中添加注释
>  kind: Service
>  metadata:
>    annotations:
>      service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
>3.kubernetes service API已经支持另外一个名为publishNotReadyAddresses的新服务规范字段，它将替换
>  tolerate-unready-endpoints注释，1.9版本还未实现
>```

#### 5.7排除服务故障

>```sh
>#如果无法通过服务访问pod，用根据下面列表进行排查
>1.首先，确保从集群内连接到服务的集群IP，而不是从外部
>2.服务的集群IP是虚拟IP，无法ping通，不要ping来判断service是否可以访问呢
>3.如果已经定义了就绪探针，请确保它返回成功；否则pod不会成为服务的一部分
>4.确认某个容器是服务的一部分，请使用kubectl get endpoints来检查相应端点对象
>5.如果尝试通过FQDN或其中一部分来访问服务（如：myservice.mynamespace.svc.cluster.local或       
>  myservice.mynamespace），但并不起作用，请查看是否可以使用其集群IP而不是FQDN来访问服务
>6.检查是否连接到服务公开端口，而不是目标端口
>7.尝试直接连接到pod IP以确认pod正在接收正确端口上的连接
>8.如果甚至无法通过pod的IP访问应用，请确保应用不是仅绑定到本地主机
>```

## 6.卷：将磁盘挂在到容器

#### 6.1介绍卷

###### 6.1.1卷的应用示例

>```sh
>1.卷是pod的一部分，拥有相同的生命周期
>2.容器重启，卷的内容保持不变，重启之后，新容器可以识别前一个容器写入卷的所有文件
>3.一个pod包含多个容器，那这个卷可以同时被所有容器使用
>4.需要将卷先挂载在需要使用它的容器中
>5.在每个容器中，都可以在其文件系统的任意位置挂载卷
>6.假设：三个容器的一个pod
>	1.第一个容器为web服务器，web服务器的html页面目录位于/var/htdocs，将访问日志存在/var/logs目录中
>	2.第二个容器运行了一个代理来创建html文件，存放在/var/html中
>	3.第三个容器处理/var/logs目录中找到的日志（转换、压缩、分析它们或做其他处理）
>	4.有这三个容器，但是没有挂在卷的pod基本上什么都做不了
>7.
>        
>```
>
>


























































































































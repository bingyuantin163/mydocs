## Kubernetes集群搭建-kubeadm

#### 1.设置ssh使服务器之间互信

###### 1.1生成公钥私钥

>```sh
>ssh-keygen -t rsa -f "xxx.xxx"  -C "yyy"
>```

###### 1.2拷贝公钥到远程主机

>```sh
>ssh-copy-id -i /root/.ssh/id_rsa.pub root@ip
>#自动拷贝到远程主机到的/root/.ssh/authorized_keys
>```

#### 2.关闭Selinux和Firewall

###### 2.1关闭selinux

>```sh
>setenforce 0
>vim /etc/selinux/config
>SELINUX=disabled
>#重启即可永久生效
>```

###### 2.2关闭firewall

>```sh
>systemctl stop firewalld
>systemctl disable firewalld
>```

#### 3.安装组件（所有机器）

![image-20191226132524832](C:\Users\Owner\AppData\Roaming\Typora\typora-user-images\image-20191226132524832.png)

​							  kubernetes架构图

![image-20191226132634719](C:\Users\Owner\AppData\Roaming\Typora\typora-user-images\image-20191226132634719.png)

​		kubernetes安装示意图

###### 3.1安装docker

>```sh
>yum install -y yum-utils device-mapper-persistent-data lvm2
>yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
>yum list docker-ce --showduplicates | sort -r
>yum -y install docker-ce
>docker --version
>systemctl start docker
>systemctl status docker
>systemctl enable docker
>```

###### 3.2安装kubelet、kubeadm、kubectl

>```sh
>1.设置仓库
>cat >> /etc/yum.repos.d/kubernetes.repo << EOF
>[kubernetes]
>name=kubernetes
>baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
>enable=1
>gpgcheck=0
>gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg
>EOF
>```
>
>```sh
>2.执行安装命令
>yum -y remove kubelet kubeadm kubectl
>yum -y install kubelet kubeadm kubectl #在这里用的是下面的1.14.2版本
>yum -y install kubeadm-1.14.2-0.x86_64 kubelet-1.14.2-0.x86_64 kubectl-1.14.2-0.x86_64
>systemctl start kubelet
>systemctl enable kubelet
>```

#### 4.安装镜像（所有机器）

###### 4.1拉取镜像

>```sh
>docker pull mirrorgooglecontainers/kube-apiserver:v1.14.2
>docker pull mirrorgooglecontainers/kube-controller-manager:v1.14.2
>docker pull mirrorgooglecontainers/kube-scheduler:v1.14.2
>docker pull mirrorgooglecontainers/kube-proxy:v1.14.2
>docker pull mirrorgooglecontainers/pause:3.1
>docker pull mirrorgooglecontainers/etcd:3.3.10
>docker pull coredns/coredns:1.3.1
>docker pull registry.cn-shenzhen.aliyuncs.com/cp_m/flannel:v0.10.0-amd64
>```

###### 4.2取别名

>```sh
>docker tag mirrorgooglecontainers/kube-apiserver:v1.14.2 k8s.gcr.io/kube-apiserver:v1.14.2
>docker tag mirrorgooglecontainers/kube-controller-manager:v1.14.2 
>                       k8s.gcr.io/kube-controller-manager:v1.14.2
>docker tag mirrorgooglecontainers/kube-scheduler:v1.14.2 k8s.gcr.io/kube-scheduler:v1.14.2
>docker tag mirrorgooglecontainers/kube-proxy:v1.14.2 k8s.gcr.io/kube-proxy:v1.14.2
>docker tag mirrorgooglecontainers/pause:3.1 k8s.gcr.io/pause:3.1
>docker tag mirrorgooglecontainers/etcd:3.3.10 k8s.gcr.io/etcd:3.3.10
>docker tag coredns/coredns:1.3.1 k8s.gcr.io/coredns:1.3.1
>docker tag registry.cn-shenzhen.aliyuncs.com/cp_m/flannel:v0.10.0-amd64         
>	                               quay.io/coreos/flannel:v0.10.0-amd64
>```

###### 4.3删除镜像

>```sh
>docker rmi mirrorgooglecontainers/kube-apiserver:v1.14.2
>docker rmi mirrorgooglecontainers/kube-controller-manager:v1.14.2
>docker rmi mirrorgooglecontainers/kube-scheduler:v1.14.2
>docker rmi mirrorgooglecontainers/kube-proxy:v1.14.2
>docker rmi mirrorgooglecontainers/pause:3.1
>docker rmi mirrorgooglecontainers/etcd:3.3.10
>docker rmi coredns/coredns:1.3.1
>docker rmi registry.cn-shenzhen.aliyuncs.com/cp_m/flannel:v0.10.0-amd64
>```

#### 5.安装master

###### 5.1初始化

>```sh
>1.#关闭交换分区，如果开启了swap分区，kubelet会启动失败（可以通过将参数 --fail-swap-on 设置为false 来忽略 swap on）
>swapoff -a
>2.#设置参数为--fail-swap-on
>vim /etc/sysconfig/kubelet
>KUBELET_EXTRA_ARGS="--fail-swap-on=false"
>KUBE_PROXY=MODE=ipvs
>3.#防止开机自动挂载swap分区，可以注释/etc/fstab中相应的条目
>sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
>4.#cpu必须大于等于2
>5.#sysctl net.ipv4.ip_forward=1
>6.#执行初始化命令
>kubeadm init --kubernetes-version=v1.14.2 --pod-network-cidr=10.244.0.0/16 
>				--service-cidr=10.96.0.0/12 --ignore-preflight-errors=Swap
>--pod-network-cidr：pod所使用的网络范围，依赖于使用的网络，这里用的是经典的flannel网络方案
>--service-cidr：指定service网段
>--ignore-preflight-errors=Swap/all：忽略swap/所有 报错
>```

###### 5.2设置      .kube/config

>```sh
>mkdir -p $HOME/.kube
>sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
>sudo chown $(id -u):$(id -g) $HOME/.kube/config
>```

###### 5.3保存输出的 kubeadm jion 行命令，在node节点需要执行

>```sh
>kubeadm join 192.168.168.131:6443 --token 3xl2g9.88wtub88mwvoqpvf \
>    --discovery-token-ca-cert-hash sha256:49a56b1bd3e3fa1cb63f146db21ffc20f9dc33e1134e0882731d91278bd0dcb2
>#若忘记保存，可以执行下面命令显示
>kubeadm token create --print-join-command
>```

###### 5.4配置kubectl

>```sh
>echo "export KUBECONFIG=/etc/kubernetes/admin.conf" >> /etc/profile
>source /etc/profile
>echo $KUBECONFIG
>```

###### 5.5安装pod网络

>```sh
>#pod网络是pod之间通信的必要条件，这里选择flannel方案
>```
>
>```sh
>1.在任意位置新建文件 kube-flannel.yaml
>vim /etc/k8s/kube-flannel.yaml
>---
>kind: ClusterRole
>apiVersion: rbac.authorization.k8s.io/v1beta1
>metadata:
>  name: flannel
>rules:
>  - apiGroups:
>      - ""
>    resources:
>      - pods
>    verbs:
>      - get
>  - apiGroups:
>      - ""
>    resources:
>      - nodes
>    verbs:
>      - list
>      - watch
>  - apiGroups:
>      - ""
>    resources:
>      - nodes/status
>    verbs:
>      - patch
>---
>kind: ClusterRoleBinding
>apiVersion: rbac.authorization.k8s.io/v1beta1
>metadata:
>  name: flannel
>roleRef:
>  apiGroup: rbac.authorization.k8s.io
>  kind: ClusterRole
>  name: flannel
>subjects:
>- kind: ServiceAccount
>  name: flannel
>  namespace: kube-system
>---
>apiVersion: v1
>kind: ServiceAccount
>metadata:
>  name: flannel
>  namespace: kube-system
>---
>kind: ConfigMap
>apiVersion: v1
>metadata:
>  name: kube-flannel-cfg
>  namespace: kube-system
>  labels:
>    tier: node
>    app: flannel
>data:
>  cni-conf.json: |
>    {
>      "name": "cbr0",
>      "plugins": [
>        {
>          "type": "flannel",
>          "delegate": {
>            "hairpinMode": true,
>            "isDefaultGateway": true
>          }
>        },
>        {
>          "type": "portmap",
>          "capabilities": {
>            "portMappings": true
>          }
>        }
>      ]
>    }
>  net-conf.json: |
>    {
>      "Network": "10.244.0.0/16",
>      "Backend": {
>        "Type": "vxlan"
>      }
>    }
>---
>apiVersion: extensions/v1beta1
>kind: DaemonSet
>metadata:
>  name: kube-flannel-ds-amd64
>  namespace: kube-system
>  labels:
>    tier: node
>    app: flannel
>spec:
>  template:
>    metadata:
>      labels:
>        tier: node
>        app: flannel
>    spec:
>      hostNetwork: true
>      nodeSelector:
>        beta.kubernetes.io/arch: amd64
>      tolerations:
>      - operator: Exists
>        effect: NoSchedule
>      serviceAccountName: flannel
>      initContainers:
>      - name: install-cni
>        image: quay.io/coreos/flannel:v0.10.0-amd64
>        command:
>        - cp
>        args:
>        - -f
>        - /etc/kube-flannel/cni-conf.json
>        - /etc/cni/net.d/10-flannel.conflist
>        volumeMounts:
>        - name: cni
>          mountPath: /etc/cni/net.d
>        - name: flannel-cfg
>          mountPath: /etc/kube-flannel/
>      containers:
>      - name: kube-flannel
>        image: quay.io/coreos/flannel:v0.10.0-amd64
>        command:
>        - /opt/bin/flanneld
>        args:
>        - --ip-masq
>        - --kube-subnet-mgr
>        resources:
>          requests:
>            cpu: "100m"
>            memory: "50Mi"
>          limits:
>            cpu: "100m"
>            memory: "50Mi"
>        securityContext:
>          privileged: true
>        env:
>        - name: POD_NAME
>          valueFrom:
>            fieldRef:
>              fieldPath: metadata.name
>        - name: POD_NAMESPACE
>          valueFrom:
>            fieldRef:
>              fieldPath: metadata.namespace
>        volumeMounts:
>        - name: run
>          mountPath: /run
>        - name: flannel-cfg
>          mountPath: /etc/kube-flannel/
>      volumes:
>        - name: run
>          hostPath:
>            path: /run
>        - name: cni
>          hostPath:
>            path: /etc/cni/net.d
>        - name: flannel-cfg
>          configMap:
>            name: kube-flannel-cfg
>---
>apiVersion: extensions/v1beta1
>kind: DaemonSet
>metadata:
>  name: kube-flannel-ds-arm64
>  namespace: kube-system
>  labels:
>    tier: node
>    app: flannel
>spec:
>  template:
>    metadata:
>      labels:
>        tier: node
>        app: flannel
>    spec:
>      hostNetwork: true
>      nodeSelector:
>        beta.kubernetes.io/arch: arm64
>      tolerations:
>      - operator: Exists
>        effect: NoSchedule
>      serviceAccountName: flannel
>      initContainers:
>      - name: install-cni
>        image: quay.io/coreos/flannel:v0.10.0-arm64
>        command:
>        - cp
>        args:
>        - -f
>        - /etc/kube-flannel/cni-conf.json
>        - /etc/cni/net.d/10-flannel.conflist
>        volumeMounts:
>        - name: cni
>          mountPath: /etc/cni/net.d
>        - name: flannel-cfg
>          mountPath: /etc/kube-flannel/
>      containers:
>      - name: kube-flannel
>        image: quay.io/coreos/flannel:v0.10.0-arm64
>        command:
>        - /opt/bin/flanneld
>        args:
>        - --ip-masq
>        - --kube-subnet-mgr
>        resources:
>          requests:
>            cpu: "100m"
>            memory: "50Mi"
>          limits:
>            cpu: "100m"
>            memory: "50Mi"
>        securityContext:
>          privileged: true
>        env:
>        - name: POD_NAME
>          valueFrom:
>            fieldRef:
>              fieldPath: metadata.name
>        - name: POD_NAMESPACE
>          valueFrom:
>            fieldRef:
>              fieldPath: metadata.namespace
>        volumeMounts:
>        - name: run
>          mountPath: /run
>        - name: flannel-cfg
>          mountPath: /etc/kube-flannel/
>      volumes:
>        - name: run
>          hostPath:
>            path: /run
>        - name: cni
>          hostPath:
>            path: /etc/cni/net.d
>        - name: flannel-cfg
>          configMap:
>            name: kube-flannel-cfg
>---
>apiVersion: extensions/v1beta1
>kind: DaemonSet
>metadata:
>  name: kube-flannel-ds-arm
>  namespace: kube-system
>  labels:
>    tier: node
>    app: flannel
>spec:
>  template:
>    metadata:
>      labels:
>        tier: node
>        app: flannel
>    spec:
>      hostNetwork: true
>      nodeSelector:
>        beta.kubernetes.io/arch: arm
>      tolerations:
>      - operator: Exists
>        effect: NoSchedule
>      serviceAccountName: flannel
>      initContainers:
>      - name: install-cni
>        image: quay.io/coreos/flannel:v0.10.0-arm
>        command:
>        - cp
>        args:
>        - -f
>        - /etc/kube-flannel/cni-conf.json
>        - /etc/cni/net.d/10-flannel.conflist
>        volumeMounts:
>        - name: cni
>          mountPath: /etc/cni/net.d
>        - name: flannel-cfg
>          mountPath: /etc/kube-flannel/
>      containers:
>      - name: kube-flannel
>        image: quay.io/coreos/flannel:v0.10.0-arm
>        command:
>        - /opt/bin/flanneld
>        args:
>        - --ip-masq
>        - --kube-subnet-mgr
>        resources:
>          requests:
>            cpu: "100m"
>            memory: "50Mi"
>          limits:
>            cpu: "100m"
>            memory: "50Mi"
>        securityContext:
>          privileged: true
>        env:
>        - name: POD_NAME
>          valueFrom:
>            fieldRef:
>              fieldPath: metadata.name
>        - name: POD_NAMESPACE
>          valueFrom:
>            fieldRef:
>              fieldPath: metadata.namespace
>        volumeMounts:
>        - name: run
>          mountPath: /run
>        - name: flannel-cfg
>          mountPath: /etc/kube-flannel/
>      volumes:
>        - name: run
>          hostPath:
>            path: /run
>        - name: cni
>          hostPath:
>            path: /etc/cni/net.d
>        - name: flannel-cfg
>          configMap:
>            name: kube-flannel-cfg
>---
>apiVersion: extensions/v1beta1
>kind: DaemonSet
>metadata:
>  name: kube-flannel-ds-ppc64le
>  namespace: kube-system
>  labels:
>    tier: node
>    app: flannel
>spec:
>  template:
>    metadata:
>      labels:
>        tier: node
>        app: flannel
>    spec:
>      hostNetwork: true
>      nodeSelector:
>        beta.kubernetes.io/arch: ppc64le
>      tolerations:
>      - operator: Exists
>        effect: NoSchedule
>      serviceAccountName: flannel
>      initContainers:
>      - name: install-cni
>        image: quay.io/coreos/flannel:v0.10.0-ppc64le
>        command:
>        - cp
>        args:
>        - -f
>        - /etc/kube-flannel/cni-conf.json
>        - /etc/cni/net.d/10-flannel.conflist
>        volumeMounts:
>        - name: cni
>          mountPath: /etc/cni/net.d
>        - name: flannel-cfg
>          mountPath: /etc/kube-flannel/
>      containers:
>      - name: kube-flannel
>        image: quay.io/coreos/flannel:v0.10.0-ppc64le
>        command:
>        - /opt/bin/flanneld
>        args:
>        - --ip-masq
>        - --kube-subnet-mgr
>        resources:
>          requests:
>            cpu: "100m"
>            memory: "50Mi"
>          limits:
>            cpu: "100m"
>            memory: "50Mi"
>        securityContext:
>          privileged: true
>        env:
>        - name: POD_NAME
>          valueFrom:
>            fieldRef:
>              fieldPath: metadata.name
>        - name: POD_NAMESPACE
>          valueFrom:
>            fieldRef:
>              fieldPath: metadata.namespace
>        volumeMounts:
>        - name: run
>          mountPath: /run
>        - name: flannel-cfg
>          mountPath: /etc/kube-flannel/
>      volumes:
>        - name: run
>          hostPath:
>            path: /run
>        - name: cni
>          hostPath:
>            path: /etc/cni/net.d
>        - name: flannel-cfg
>          configMap:
>            name: kube-flannel-cfg
>---
>apiVersion: extensions/v1beta1
>kind: DaemonSet
>metadata:
>  name: kube-flannel-ds-s390x
>  namespace: kube-system
>  labels:
>    tier: node
>    app: flannel
>spec:
>  template:
>    metadata:
>      labels:
>        tier: node
>        app: flannel
>    spec:
>      hostNetwork: true
>      nodeSelector:
>        beta.kubernetes.io/arch: s390x
>      tolerations:
>      - operator: Exists
>        effect: NoSchedule
>      serviceAccountName: flannel
>      initContainers:
>      - name: install-cni
>        image: quay.io/coreos/flannel:v0.10.0-s390x
>        command:
>        - cp
>        args:
>        - -f
>        - /etc/kube-flannel/cni-conf.json
>        - /etc/cni/net.d/10-flannel.conflist
>        volumeMounts:
>        - name: cni
>          mountPath: /etc/cni/net.d
>        - name: flannel-cfg
>          mountPath: /etc/kube-flannel/
>      containers:
>      - name: kube-flannel
>        image: quay.io/coreos/flannel:v0.10.0-s390x
>        command:
>        - /opt/bin/flanneld
>        args:
>        - --ip-masq
>        - --kube-subnet-mgr
>        resources:
>          requests:
>            cpu: "100m"
>            memory: "50Mi"
>          limits:
>            cpu: "100m"
>            memory: "50Mi"
>        securityContext:
>          privileged: true
>        env:
>        - name: POD_NAME
>          valueFrom:
>            fieldRef:
>              fieldPath: metadata.name
>        - name: POD_NAMESPACE
>          valueFrom:
>            fieldRef:
>              fieldPath: metadata.namespace
>        volumeMounts:
>        - name: run
>          mountPath: /run
>        - name: flannel-cfg
>          mountPath: /etc/kube-flannel/
>      volumes:
>        - name: run
>          hostPath:
>            path: /run
>        - name: cni
>          hostPath:
>            path: /etc/cni/net.d
>        - name: flannel-cfg
>          configMap:
>            name: kube-flannel-cfg
>```
>
>```sh
>2.设置系统参数
>sysctl net.bridge.bridge-nf-call-iptables=1
>```
>
>```sh
>3.使用kube-flannel.yaml文件，创建pod网络flannel
>kubectl apply -f kube-flannel.yaml
>```
>
>```sh
>4.检查pod网络是否正常
>kubectl get pods --all-namespaces -o wide
>kubectl get pods -n kube-system
>#通常这里出现问题
>kubectl describe pod xxx -n  kube-system
>报错：Readiness probe failed: HTTP probe failed with statuscode: 503
>kubectl logs -n kube-system  ${POD_NAME}
>报错：Failed to list *v1.Namespace
>建议查看防火墙设置
>```
>
>```sh
>5.查看节点状态
>kubectl get nodes
>```

#### 6.添加node节点

###### 6.1执行kubeadm join

>```sh
>1.查看kubeadm join的完整命令
>kubeadm token create --print-join-command
>kubeadm token list
>2.执行查找出来的kubeadm join
>kubeadm join 192.168.168.131:6443 --token 0e7rfr.oip1eyru0v1bxh00     --discovery-token-ca-cert-hash sha256:4eb61153134bc92ccd1947af6b7036908cd2050821991aff6a17f18ff4fad64f 
>```

###### 6.2查看校验节点状态

>```sh
>kubectl get nodes -o wide
>```

###### 6.3查看所有pod状态

>```sh
>kubectl get pods --all-namespaces -o wide
>```

#### 7.迁移pod

###### 7.1设置节点是否可调度

>```sh
>#确定需要迁移和被迁移的节点，将不被允许迁移的节点设置为不可调度
>1.节点设置为不可调度
>kubectl cordon <NodeName>
>2.节点设置为可调度
>kubectl cordon <NodeName>
>```

###### 7.2执行kubectl drain命令

>```sh
>1.kubectl drain <NodeName> --force --ignore-daemonsets
>2.删除相应节点上的旧pod，并在可调度节点上面起一个对应的pod
>3.当旧pod没有被正常删除情况下，新pod不会起来，如：旧pod一直处于Terminating状态
>4.对应的解决方式是重启相应节点的kubelet，或者强制删除pod
>	systemctl restart kubelet
>	kubectl delete pod <PodName> --namespace=<Namespace> --force --grace-period-0
>```

#### 8.在master节点上安装dashboard（v1.10.0）

###### 8.1下载镜像

>```sh
>1.查看对象的几条命令
>kubectl api-resources #通过这个查看出资源对应的APIGROUP
>kubectl api-versions  #查看APIGROUP可以使用的apiVersion的版本
>kubectl explain ${RESOURCE_NAME}  #查看系统对资源的详细解释
>```
>
>```sh
>2.下载镜像
>docker pull registry.cn-qingdao.aliyuncs.com/wangxiaoke/kubernetes-dashboard-amd64:v1.10.0
>docker tag registry.cn-qingdao.aliyuncs.com/wangxiaoke/kubernetes-dashboard-amd64:v1.10.0                k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.0
>docker rmi registry.cn-qingdao.aliyuncs.com/wangxiaoke/kubernetes-dashboard-amd64:v1.10.0
>#此处将kubernetes-dashboard-amd64:v1.10.0上传到自己的公开镜像仓库，后面需要可以直接下载
>#docker login
>#docker tag k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.0 bingyuantin/kubernete-dashboard-amd64:v1.10.0
>#docker push bingyuantin/kubernete-dashboard-amd64:v1.10.0
>#docker rmi bingyuantin/kubernete-dashboard-amd64:v1.10.0
>```

###### 8.2安装dashboard

>```sh
>vim /etc/k8s/kubernetes-dashboard.yaml
># Copyright 2017 The Kubernetes Authors.
>#
># Licensed under the Apache License, Version 2.0 (the "License");
># you may not use this file except in compliance with the License.
># You may obtain a copy of the License at
>#
>#     http://www.apache.org/licenses/LICENSE-2.0
>#
># Unless required by applicable law or agreed to in writing, software
># distributed under the License is distributed on an "AS IS" BASIS,
># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
># See the License for the specific language governing permissions and
># limitations under the License.
>
># ------------------- Dashboard Secret ------------------- #
>
>apiVersion: v1
>kind: Secret
>metadata:
>  labels:
>    k8s-app: kubernetes-dashboard
>  name: kubernetes-dashboard-certs
>  namespace: kube-system
>type: Opaque
>
>---
># ------------------- Dashboard Service Account ------------------- #
>
>apiVersion: v1
>kind: ServiceAccount
>metadata:
>  labels:
>    k8s-app: kubernetes-dashboard
>  name: kubernetes-dashboard
>  namespace: kube-system
>
>---
># ------------------- Dashboard Role & Role Binding ------------------- #
>
>kind: Role
>apiVersion: rbac.authorization.k8s.io/v1
>metadata:
>  name: kubernetes-dashboard-minimal
>  namespace: kube-system
>rules:
>  # Allow Dashboard to create 'kubernetes-dashboard-key-holder' secret.
>- apiGroups: [""]
>  resources: ["secrets"]
>  verbs: ["create"]
>  # Allow Dashboard to create 'kubernetes-dashboard-settings' config map.
>- apiGroups: [""]
>  resources: ["configmaps"]
>  verbs: ["create"]
>  # Allow Dashboard to get, update and delete Dashboard exclusive secrets.
>- apiGroups: [""]
>  resources: ["secrets"]
>  resourceNames: ["kubernetes-dashboard-key-holder", "kubernetes-dashboard-certs"]
>  verbs: ["get", "update", "delete"]
>  # Allow Dashboard to get and update 'kubernetes-dashboard-settings' config map.
>- apiGroups: [""]
>  resources: ["configmaps"]
>  resourceNames: ["kubernetes-dashboard-settings"]
>  verbs: ["get", "update"]
>  # Allow Dashboard to get metrics from heapster.
>- apiGroups: [""]
>  resources: ["services"]
>  resourceNames: ["heapster"]
>  verbs: ["proxy"]
>- apiGroups: [""]
>  resources: ["services/proxy"]
>  resourceNames: ["heapster", "http:heapster:", "https:heapster:"]
>  verbs: ["get"]
>
>---
>apiVersion: rbac.authorization.k8s.io/v1
>kind: RoleBinding
>metadata:
>  name: kubernetes-dashboard-minimal
>  namespace: kube-system
>roleRef:
>  apiGroup: rbac.authorization.k8s.io
>  kind: Role
>  name: kubernetes-dashboard-minimal
>subjects:
>- kind: ServiceAccount
>  name: kubernetes-dashboard
>  namespace: kube-system
>
>---
># ------------------- Dashboard Deployment ------------------- #
>
>kind: Deployment
>apiVersion: apps/v1beta2
>metadata:
>  labels:
>    k8s-app: kubernetes-dashboard
>  name: kubernetes-dashboard
>  namespace: kube-system
>spec:
>  replicas: 1
>  revisionHistoryLimit: 10
>  selector:
>    matchLabels:
>      k8s-app: kubernetes-dashboard
>  template:
>    metadata:
>      labels:
>        k8s-app: kubernetes-dashboard
>    spec:
>      containers:
>      - name: kubernetes-dashboard
>        image: k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.0
>        ports:
>        - containerPort: 8443
>          protocol: TCP
>        args:
>          - --auto-generate-certificates
>          # Uncomment the following line to manually specify Kubernetes API server Host
>          # If not specified, Dashboard will attempt to auto discover the API server and connect
>          # to it. Uncomment only if the default does not work.
>          # - --apiserver-host=http://my-address:port
>        volumeMounts:
>        - name: kubernetes-dashboard-certs
>          mountPath: /certs
>          # Create on-disk volume to store exec logs
>        - mountPath: /tmp
>          name: tmp-volume
>        livenessProbe:
>          httpGet:
>            scheme: HTTPS
>            path: /
>            port: 8443
>          initialDelaySeconds: 30
>          timeoutSeconds: 30
>      volumes:
>      - name: kubernetes-dashboard-certs
>        hostPath:
>          path: /home/share/certs
>          type: Directory
>        # secret:
>          # secretName: kubernetes-dashboard-certs
>      - name: tmp-volume
>        emptyDir: {}
>      serviceAccountName: kubernetes-dashboard
>      # Comment the following tolerations if Dashboard must not be deployed on master
>      tolerations:
>      - key: node-role.kubernetes.io/master
>        effect: NoSchedule
>
>---
># ------------------- Dashboard Service ------------------- #
>
>kind: Service
>apiVersion: v1
>metadata:
>  labels:
>    k8s-app: kubernetes-dashboard
>  name: kubernetes-dashboard
>  namespace: kube-system
>spec:
>  ports:
>    - port: 443
>      targetPort: 8443
>      nodePort: 31234
>  selector:
>    k8s-app: kubernetes-dashboard
>  type: NodePort
>```
>
>```sh
>1.官方版本 https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.0/src/deploy/recommended/kubernetes-dashboard.yaml
>2.注意NodePort及hostPath设置与官方版本不同之处
>3.kubectl create -f /etc/k8s/kubernetes-dashboard.yaml
>```

###### 8.3查看dashboard的pod是否正常运行

>```sh
>1.kubectl get pods --all-namespaces -o wide
>2.如果状态是：ContainerCreating
>  通过kubectl describe pod kubernetes-dashboard-xxx-yyy --namespace=kube-system查看原因
>3.master和node创建需要放置证书的目录
>  mkdir -p /home/share/certs/
>4.排除错误后，重新执行
>  kubectl delete -f kubernetes-dashboard.yaml  #如果提示文件存在，执行此命令清楚文件
>  kubectl create -f kubernetes-dashboard.yaml  #重新运行一个pod
>```

###### 8.4查看dashboard的外网暴露端口

>```sh
>kubectl get service --namespace=kube-system -o wide 
>```
>
>```sh
>NAME                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                  AGE 
>kube-dns             ClusterIP   10.96.0.10       <none>        53/UDP,53/TCP,9153/TCP   2d2h
>kubernetes-dashboard NodePort    10.101.180.120   <none>        443:31234/TCP            80m
>```
>
>```sh
>#其中的31234即为外网接口，后面访问dashboard会用到
>```

###### 8.5生成私钥和证书签名在master节点执行, 如果有输入或者选择, 直接回车

>```sh
>1.生成RSA私钥（使用des3加密，最后会对加密操作和命令做一些记录及注释）
>  openssl genrsa -des3 -passout pass:x	-out  dashboard.pass.key  2048
>2.将加密的私钥转换成不加密的私钥
>  openssl rsa -passin pass:x -in dashboard.pass.key -out dashboard.key
>3.删除加密私钥
>  rm -rf dashboard.pass.key
>4.生成证书签名请求文件
>  openssl req -new -key dashboard.key -out dashboard.csr #全部回车即可
>5.利用已有的RSA私钥生成自签名的加密证书
>openssl x509 -req -sha256 -days 365 -in dashboard.csr -signkey dashboard.key -out dashboard.crt	
>6.将证书和私钥传到/home/share/certs
>  cp  dashboard.crt dashboard.key  /home/share/certs/
>```

###### 8.6创建`dashbord`用户

>```sh
>vim /etc/k8s/dashboard-user-role.yaml
>kind: ClusterRoleBinding
>apiVersion: rbac.authorization.k8s.io/v1beta1
>metadata:
>  name: admin
>  annotations:
>    rbac.authorization.kubernetes.io/autoupdate: "true"
>roleRef:
>  kind: ClusterRole
>  name: cluster-admin
>  apiGroup: rbac.authorization.k8s.io
>subjects:
>- kind: ServiceAccount
>  name: admin
>  namespace: kube-system
>---
>apiVersion: v1
>kind: ServiceAccount
>metadata:
>  name: admin
>  namespace: kube-system
>  labels:
>    kubernetes.io/cluster-service: "true"
>    addonmanager.kubernetes.io/mode: Reconcile
>```
>
>```sh
>kubectl create -f dashboard-user-role.yaml
>```

###### 8.7获取登录token，如果忘了可以执行下面命令

>```sh
>kubectl describe secret/$(kubectl get secret -nkube-system |grep admin|awk '{print $1}') -nkube-system
>```

###### 8.8登录

>```sh
>#建议使用firefox浏览器
>https://masterIp:31234/#!/settings?namespace=default
>```
>
>```sh
>#选择token令牌登录
>```




























5.Kubernetes集群搭建

```http
官网地址：https://kubernetes.io/
类似参考文档地址：http://www.louisvv.com/archives/1127.html
```

1.节点准备

| IP              | 角色                      | 主要组件                                                     |
| :-------------- | ------------------------- | ------------------------------------------------------------ |
| 192.168.168.129 | master：kubernetes-master | kube-apiserver，kube-apiserver，kube-controller-manager，kube-scheduler，kubelet，etcd |
| 192.168.168.128 | node01：kubernetes-node   | docker，kubelet，kube-proxy，flannel                         |
| 192.168.168.130 | node02：kubernetes-node   | docker，kubelet，kube-proxy，flannel                         |

2.配置yum源

2.1国外配置kubernetes yum源：

```shell
cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=http://yum.kubernetes.io/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=0
EOF
```

2.2国内配置yum源

```shell
#docker yum源
cat >> /etc/yum.repos.d/docker.repo <<EOF
[docker-repo]
name=Docker Repository
baseurl=http://mirrors.aliyun.com/docker-engine/yum/repo/main/centos/7
enabled=1
gpgcheck=0
EOF

#kubernetes yum源
cat >> /etc/yum.repos.d/kubernetes.repo <<EOF
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=0
EOF
```

3.关闭防火墙、Selinux、配置主机/etc/hosts文件

```shell
vim /etc/hosts
192.168.168.128  vm1
192.168.168.129  vm2
192.168.168.130  vm3
```

4.在主节点上安装etcd

```shell
yum -y install etcd
```

```shell
vim /etc/etcd/etcd.conf
ETCD_DATA_DIR="/var/lib/etcd/default.etcd"
ETCD_LISTEN_CLIENT_URLS="http://0.0.0.0:2379,http://0.0.0.0:4001"
ETCD_NAME="master"
ETCD_ADVERTISE_CLIENT_URLS="http://vm2:2379,http://vm2:4001"
```

```shell
systemctl restart etcd
systemctl enable etcd 
```

```shell
# 验证状态
etcdctl -C http://vm2:4001 cluster-health
etcdctl -C http://vm2:2379 cluster-health
```

5.安装kubernetes

5.1Master 节点

```shell
yum -y install kubernetes
# yum安装过程如果中出现下面问题
docker-ce-cli conflicts with 2:docker-1.13.1-103.git7f2769b.el7.centos.x86_64
docker-ce conflicts with 2:docker-1.13.1-103.git7f2769b.el7.centos.x86_64
# 之前安装过docker，先卸载，再重新安装kubernetes
yum list installed | grep docker
yum -y remove  containerd.io.x86_64  docker-ce.x86_64  docker-ce-cli.x86_64
yum -y install kubernetes
```

```shell
cd /etc/kubernetes
```

```shell
[root@vm2 kubernetes]# vim config
# kubernetes system config
#
# The following values are used to configure various aspects of all
# kubernetes services, including
#
#   kube-apiserver.service
#   kube-controller-manager.service
#   kube-scheduler.service
#   kubelet.service
#   kube-proxy.service
# logging to stderr means we get it in the systemd journal
KUBE_LOGTOSTDERR="--logtostderr=true"

# journal message level, 0 is debug
KUBE_LOG_LEVEL="--v=0"

# Should this cluster be allowed to run privileged docker containers
KUBE_ALLOW_PRIV="--allow-privileged=false"

# How the controller-manager, scheduler, and proxy find the apiserver
#KUBE_MASTER="--master=http://127.0.0.1:8080"
KUBE_MASTER="--master=http://vm2:8080"
KUBE_ETCD_SERVERS="--etcd_servers=http://vm2:4001"

```

```shell
 [root@vm2 kubernetes]# vim apiserver
# kubernetes system config
#
# The following values are used to configure the kube-apiserver
#

# The address on the local server to listen to.
KUBE_API_ADDRESS="--insecure-bind-address=0.0.0.0"

# The port on the local server to listen on.
KUBE_API_PORT="--port=8080"

# Port minions listen on
# KUBELET_PORT="--kubelet-port=10250"

# Comma separated list of nodes in the etcd cluster
KUBE_ETCD_SERVERS="--etcd-servers=http://vm2:2379"

# Address range to use for services
KUBE_SERVICE_ADDRESSES="--service-cluster-ip-range=10.254.0.0/16"

# default admission control policies
KUBE_ADMISSION_CONTROL="--admission-control=NamespaceLifecycle,NamespaceExists,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota"

# Add your own!
KUBE_API_ARGS=""
```

```shell
systemctl restart kube-apiserver	systemctl enable kube-apiserver
systemctl restart kube-controller-manager	systemctl enable kube-controller-manager
systemctl restart kube-scheduler	systemctl enable kube-scheduler
ps -ef | grep kube
```

5.2Node 节点（2个）

```shell
yum -y install kubernetes
# yum安装过程如果中出现下面问题
docker-ce-cli conflicts with 2:docker-1.13.1-103.git7f2769b.el7.centos.x86_64
docker-ce conflicts with 2:docker-1.13.1-103.git7f2769b.el7.centos.x86_64
# 之前安装过docker，先卸载，再重新安装kubernetes
yum list installed | grep docker
yum -y remove  containerd.io.x86_64  docker-ce.x86_64  docker-ce-cli.x86_64
yum -y install kubernetes
```

```shell
cd /etc/kubernetes
```

```shell
[root@vm1 kubernetes]# vim config
# kubernetes system config
#
# The following values are used to configure various aspects of all
# kubernetes services, including
#
#   kube-apiserver.service
#   kube-controller-manager.service
#   kube-scheduler.service
#   kubelet.service
#   kube-proxy.service
# logging to stderr means we get it in the systemd journal
KUBE_LOGTOSTDERR="--logtostderr=true"

# journal message level, 0 is debug
KUBE_LOG_LEVEL="--v=0"

# Should this cluster be allowed to run privileged docker containers
KUBE_ALLOW_PRIV="--allow-privileged=false"

# How the controller-manager, scheduler, and proxy find the apiserver
KUBE_MASTER="--master=http://vm2:8080"
KUBE_ETCD_SERVERS="--etcd_servers=http://vm2:4001"
```

```shell
[root@vm3 kubernetes]# vim config
# kubernetes system config
#
# The following values are used to configure various aspects of all
# kubernetes services, including
#
#   kube-apiserver.service
#   kube-controller-manager.service
#   kube-scheduler.service
#   kubelet.service
#   kube-proxy.service
# logging to stderr means we get it in the systemd journal
KUBE_LOGTOSTDERR="--logtostderr=true"

# journal message level, 0 is debug
KUBE_LOG_LEVEL="--v=0"

# Should this cluster be allowed to run privileged docker containers
KUBE_ALLOW_PRIV="--allow-privileged=false"

# How the controller-manager, scheduler, and proxy find the apiserver
KUBE_MASTER="--master=http://vm2:8080"
KUBE_ETCD_SERVERS="--etcd_servers=http://vm2:4001"
```

```shell
[root@vm1 kubernetes]# vim kubelet
# kubernetes kubelet (minion) config

# The address for the info server to serve on (set to 0.0.0.0 or "" for all interfaces)
KUBELET_ADDRESS="--address=0.0.0.0"

# The port for the info server to serve on
# KUBELET_PORT="--port=10250"

# You may leave this blank to use the actual hostname
KUBELET_HOSTNAME="--hostname-override=vm1"

# location of the api-server
KUBELET_API_SERVER="--api-servers=http://vm2:8080"

# pod infrastructure container
KUBELET_POD_INFRA_CONTAINER="--pod-infra-container-image=registry.access.redhat.com/rhel7/pod-infrastructure:latest"

# Add your own!
KUBELET_ARGS=""
```

```shell
[root@vm3 kubernetes]# vim kubelet
# kubernetes kubelet (minion) config

# The address for the info server to serve on (set to 0.0.0.0 or "" for all interfaces)
KUBELET_ADDRESS="--address=0.0.0.0"

# The port for the info server to serve on
# KUBELET_PORT="--port=10250"

# You may leave this blank to use the actual hostname
KUBELET_HOSTNAME="--hostname-override=vm3"

# location of the api-server
KUBELET_API_SERVER="--api-servers=http://vm2:8080"

# pod infrastructure container
KUBELET_POD_INFRA_CONTAINER="--pod-infra-container-image=registry.access.redhat.com/rhel7/pod-infrastructure:latest"

# Add your own!
KUBELET_ARGS=""
```

```shell
systemctl restart kube-proxy	systemctl enable kube-proxy
systemctl restart kubelet	systemctl enable kubelet
 ps -ef | grep kube
```

5.3查看节点信息描述

```shell
[root@vm2 ~]# kubectl describe node vm1
Name:			vm1
Role:			
Labels:			beta.kubernetes.io/arch=amd64
			beta.kubernetes.io/os=linux
			kubernetes.io/hostname=vm1
Taints:			<none>
CreationTimestamp:	Fri, 13 Dec 2019 17:42:24 +0800
Phase:			
Conditions:
  Type			Status	LastHeartbeatTime			LastTransitionTime			Reason				Message
  ----			------	-----------------			------------------			------				-------
  OutOfDisk 		False 	Sat, 14 Dec 2019 11:03:24 +0800 	Sat, 14 Dec 2019 09:52:52 +0800 	KubeletHasSufficientDisk 	kubelet has sufficient disk space available
  MemoryPressure 	False 	Sat, 14 Dec 2019 11:03:24 +0800 	Fri, 13 Dec 2019 17:42:26 +0800 	KubeletHasSufficientMemory 	kubelet has sufficient memory available
  DiskPressure 		False 	Sat, 14 Dec 2019 11:03:24 +0800 	Fri, 13 Dec 2019 17:42:26 +0800 	KubeletHasNoDiskPressure 	kubelet has no disk pressure
  Ready 		True 	Sat, 14 Dec 2019 11:03:24 +0800 	Sat, 14 Dec 2019 09:52:52 +0800 	KubeletReady 			kubelet is posting ready status
Addresses:		192.168.168.128,192.168.168.128,vm1
Capacity:
 alpha.kubernetes.io/nvidia-gpu:	0
 cpu:					1
 memory:				1863076Ki
 pods:					110
Allocatable:
 alpha.kubernetes.io/nvidia-gpu:	0
 cpu:					1
 memory:				1863076Ki
 pods:					110
System Info:
 Machine ID:			35cadfa0752d49be8d21c0415a60bf9c
 System UUID:			BE584D56-C23D-3655-EC48-00B0BBD00E00
 Boot ID:			2363e4ea-9206-4ba9-adc3-016f7d6acbbc
 Kernel Version:		3.10.0-1062.el7.x86_64
 OS Image:			CentOS Linux 7 (Core)
 Operating System:		linux
 Architecture:			amd64
 Container Runtime Version:	docker://1.13.1
 Kubelet Version:		v1.5.2
 Kube-Proxy Version:		v1.5.2
ExternalID:			vm1
Non-terminated Pods:		(0 in total)
  Namespace			Name		CPU Requests	CPU Limits	Memory Requests	Memory Limits
  ---------			----		------------	----------	---------------	-------------
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.
  CPU Requests	CPU Limits	Memory Requests	Memory Limits
  ------------	----------	---------------	-------------
  0 (0%)	0 (0%)		0 (0%)		0 (0%)
Events:
  FirstSeen	LastSeen	Count	From			SubObjectPath	Type		Reason			Message
  ---------	--------	-----	----			-------------	--------	------			-------
  43m		43m		1	{kubelet vm1}				Normal		Starting		Starting kubelet.
  43m		43m		1	{kubelet vm1}				Warning		ImageGCFailed		unable to find data for container /
  43m		43m		1	{kubelet vm1}				Normal		NodeHasSufficientDisk	Node vm1 status is now: NodeHasSufficientDisk
  43m		43m		1	{kubelet vm1}				Normal		NodeHasSufficientMemory	Node vm1 status is now: NodeHasSufficientMemory
  43m		43m		1	{kubelet vm1}				Normal		NodeHasNoDiskPressure	Node vm1 status is now: NodeHasNoDiskPressure
  43m		43m		1	{kubelet vm1}				Normal		Starting		Starting kubelet.
  43m		43m		1	{kubelet vm1}				Warning		ImageGCFailed		unable to find data for container /
  43m		43m		1	{kubelet vm1}				Normal		NodeHasSufficientDisk	Node vm1 status is now: NodeHasSufficientDisk
  43m		43m		1	{kubelet vm1}				Normal		NodeHasSufficientMemory	Node vm1 status is now: NodeHasSufficientMemory
  43m		43m		1	{kubelet vm1}				Normal		NodeHasNoDiskPressure	Node vm1 status is now: NodeHasNoDiskPressure
  42m		42m		1	{kube-proxy vm1}			Normal		Starting		Starting kube-proxy.
```

5.4创建覆盖网络

在master和node节点均执行如下命令

```shell
yum -y install flannel
```

```shell
[root@vm2 ~]# vim /etc/sysconfig/flanneld
# Flanneld configuration options  

# etcd url location.  Point this to the server where etcd runs
FLANNEL_ETCD_ENDPOINTS="http://vm2:2379"

# etcd config key.  This is the configuration key that flannel queries
# For address range assignment
FLANNEL_ETCD_PREFIX="/atomic.io/network"

# Any additional options that you want to pass
#FLANNEL_OPTIONS=""
```

在master节点执行

```shell
# 配置etcd中关于flannel的key
[root@vm2 ~]# etcdctl mk /atomic.io/network/config '{ "Network": "10.0.0.0/16" }'
{ "Network": "10.0.0.0/16" }
```

重启服务

```shell
# master上执行
systemctl restart flanneld	systemctl enable flanneld
systemctl restart docker	systemctl enable docker 
systemctl restart kube-apiserver	systemctl enable kube-apiserver
systemctl restart kube-controller-manager	systemctl enable kube-controller-manager
systemctl restart kube-scheduler	systemctl enable kube-scheduler	
```

```shell
# node上执行
systemctl restart flanneld	systemctl enable flanneld
systemctl restart docker	systemctl enable flanneld
systemctl restart kubelet	systemctl enable kubelet
systemctl restart kube-proxy	systemctl enable kube-proxy
```

5.5查看集群节点

```shell
[root@vm2 ~]# kubectl get nodes
NAME      STATUS    AGE
vm1       Ready     17h
vm3       Ready     17h
```
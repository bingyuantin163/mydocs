5.Kubernetes集群搭建

```http
官网地址：https://kubernetes.io/
类似参考文档地址：http://www.louisvv.com/archives/1127.html
```

```tex
Kubernetes是什么？

1.是一个全新的基于容器技术的分布式架构，是谷歌的Borg技术的一个开源版本

Borg是谷歌的一个久负盛名的内部使用的大规模集群管理系统，基于容器技术，目的是实现资源管理的自动化，垮多个数据中心的资源利用率的最大化

2.Kubernetes是一个开放的平台。不局限于任何一种语言，没有限定任何编程接口。

3.Kubernetes是一个完备的分布式系统支持平台。Kubernetes具有完备的集群管理能力。包括多层次的安全防护机制和准入机制，多租户应用支撑，透明的服务注册和服务发现机制，内建只能负载均衡器，强大的故障发现和自我修复能力，服务的滚动升级和在线扩容能力，可扩展的资源自动调动机制，以及多粒度的资源配额管理能力

为什么要用kubernetes？

kubernetes作为被业内认可的Docker分布式解决方案，实现微服务架构。微服务架构的核心是一个巨大的单体应用切分为多个很小的互相连接的微服务，一个微服务背后是多个实例的副本在进行支撑，副本数量可以根据系统的负荷进行调整，内嵌的负载均衡器自动实现调度。

kubernetes具备超强的横向扩容能力，可以横向扩展node数量，并且可以实现秒级的业务横向扩展，对于电商行业秒杀，拼团等等流量徒增时间和量不确定，以及大促期间需要整体进行扩容有极大的帮助，可将业务统一跑kubernetes上，而后端数据库都统一依赖云平台数据库服务，物理机或云主机自建。

大致了解了kubernetes后，我们来简单的了解一下kubernetes的基本概念

kubernetes的大部分概念，比如：node、pod、RepplicationController、Service等工具都可以看作一种“资源对象”，几乎所有的资源都可以通过kubernetes提供的kubectl工具进行增、删、改、查等操作并将其保存在etcd中持久化存储。

Master(主节点)

集群管理和控制节点，基本上kubernetes集群的所有控制命令都发给它，它来负责具体的执行过程

高可用集群需要配置三台Master

Master节点运行着以下进程：

  ● kubernetes api server提供了http rest接口的关键服务进程，是kubernetes里所有资源增、删、改、查等操作的唯一入口，也是集群控制的进程入口；

  ● kubernetes controller manager是kubernetes里所有资源对象的自动化控制中心；

  ● kubernetes scheduler负责资源调度Pod调度的进程。

在Master节点上还需启动一个etcd服务，用来保存所有资源对象的数据，在etcd中存储的时候以minion的方式存储

Node(节点)

在kubernetes集群中，除了master节点，其他节点都被称为node节点，在早期版本被称为minion

node是kubernetes集群的工作负载节点，node节点可为物理机，也可为虚拟机，每个node都会被master分配一些工作负载(docker容器)，其上的工作负载会被master主动转移到其他节点上去。

Node节点上运行着的进程

  ● kubelet负责Pod对应的容器的创建，启停等任务，同时与matser节点密切协作，实现集群管理基本功能；

  ● kube-proxy实现kubernetes service的通信与负载均衡机制；

  ● docker engine docker引擎，负责本机的容器创建和管理。

node节点会动态的添加到kubernetes集群，由kubelet负责向master进行注册；

注册成功后kubelet会向master汇报自身情况，包括操作系统，docker版本，机器cpu和内存使用

master会根据Node资源使用进行调度

如果node长时间不上报，master会判断node失联，状态会变为Not ready，master会触发工作负载转移的流程。

Pod

pod是kubernetes最重要也是最基本的概念，包含一个活或多个紧密相关的容器和卷，每个pod都有一个特殊被称为“根容器”的Pause容器，

Pause容器对应的镜像属于kubernetes平台的一部分

kubernetes为什么会有pod的概念？

1.一组容器作为一个单元，我们不好对整体进行判断和有效的操作，如果一个容器宕机，是否看做整体宕机，而pod的状态代表整个容器组的状态；

2.Pod的多个容器使用pause容器的ip，共享挂载在pause容器的valume，简化了关联容器之间的通信，并很好解决的文件共享问题。

Service(服务)

Services也是Kubernetes的基本操作单元，是真实应用服务的抽象，每一个服务后面都有很多对应的容器来支持。

一个Service可以看作一组提供相同服务的Pod的对外访问接口。

Service作用于哪些Pod是通过Label Selector来定义的。

Replication Controller(RC)(pod副本控制器)

Replication Controller确保任何时候Kubernetes集群中有指定数量的pod副本(replicas)在运行， 如果少于指定数量的pod副本(replicas)，Replication Controller会启动新的Container，反之会杀死多余的以保证数量不变。Replication Controller使用预先定义的pod模板创建pods，一旦创建成功，pod 模板和创建的pods没有任何关联，可以修改pod 模板而不会对已创建pods有任何影响，也可以直接更新通过Replication Controller创建的pods

Volume(存储卷)

Volume是Pod中能够被多个容器访问的共享目录，这个概念在Kubernetes和Docker中是类似的，Kubernetes的Volume被定义在Pod上，然后被Pod中的容器挂载，Volume的生命周期与Pod相同，Kubernetes支持多种类型的Volume，例如GlusterFS，Ceph等先进的分布式文件系统。

Label(标签)

Lable是一个key-value的键值对，需要由用户定义，附加到资源上，例如node，pod，service，rc等，每个资源可以有任意数量的label，同一个label也可以附加到任意的资源对象上，可以在资源创建的时候创建，也可以在资源创建的时候定义，也可以在创建完成后动态添加和删除。对资源创建一个或多个Label可以实现多维度的资源分组管理功能，这样可以更方便的进行资源分配，调度，配置和部署，然后通过Label Seletor查询或者筛选具有某些label的资源。

Namespace(命名空间)

Namespace是kubernetes用于实现多租户的资源隔离，将集群内部的容器对象分配到不同的Namespace中，形成逻辑上不同项目的分组，便于共享使用整个集群资源和分别管理。
```

1.节点准备

| IP              | 角色                      | 主要组件                                                     |
| :-------------- | ------------------------- | ------------------------------------------------------------ |
| 192.168.168.129 | master：kubernetes-master | kube-apiserver，kube-apiserver，kube-controller-manager，kube-scheduler，kubelet，etcd |
| 192.168.168.128 | node01：kubernetes-node   | docker，kubelet，kube-proxy，flannel                         |
| 192.168.168.130 | node02：kubernetes-node   | docker，kubelet，kube-proxy，flannel                         |

2.配置yum源

2.1国外配置kubernetes yum源：

```shell
cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=http://yum.kubernetes.io/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=0
EOF
```

2.2国内配置yum源

```shell
#docker yum源
cat >> /etc/yum.repos.d/docker.repo <<EOF
[docker-repo]
name=Docker Repository
baseurl=http://mirrors.aliyun.com/docker-engine/yum/repo/main/centos/7
enabled=1
gpgcheck=0
EOF

#kubernetes yum源
cat >> /etc/yum.repos.d/kubernetes.repo <<EOF
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=0
EOF
```

3.关闭防火墙、Selinux、配置主机/etc/hosts文件

```shell
vim /etc/hosts
192.168.168.128  vm1
192.168.168.129  vm2
192.168.168.130  vm3
```

4.在主节点上安装etcd

```shell
yum -y install etcd
```

```shell
vim /etc/etcd/etcd.conf
ETCD_DATA_DIR="/var/lib/etcd/default.etcd"
ETCD_LISTEN_CLIENT_URLS="http://0.0.0.0:2379,http://0.0.0.0:4001"
ETCD_NAME="master"
ETCD_ADVERTISE_CLIENT_URLS="http://vm2:2379,http://vm2:4001"
```

```shell
systemctl restart etcd
systemctl enable etcd 
```

```shell
# 验证状态
etcdctl -C http://vm2:4001 cluster-health
etcdctl -C http://vm2:2379 cluster-health
```

5.安装kubernetes

5.1Master 节点

```shell
yum -y install kubernetes
# yum安装过程如果中出现下面问题
docker-ce-cli conflicts with 2:docker-1.13.1-103.git7f2769b.el7.centos.x86_64
docker-ce conflicts with 2:docker-1.13.1-103.git7f2769b.el7.centos.x86_64
# 之前安装过docker，先卸载，再重新安装kubernetes
yum list installed | grep docker
yum -y remove  containerd.io.x86_64  docker-ce.x86_64  docker-ce-cli.x86_64
yum -y install kubernetes
```

```shell
cd /etc/kubernetes
```

```shell
[root@vm2 kubernetes]# vim config
# kubernetes system config
#
# The following values are used to configure various aspects of all
# kubernetes services, including
#
#   kube-apiserver.service
#   kube-controller-manager.service
#   kube-scheduler.service
#   kubelet.service
#   kube-proxy.service
# logging to stderr means we get it in the systemd journal
KUBE_LOGTOSTDERR="--logtostderr=true"

# journal message level, 0 is debug
KUBE_LOG_LEVEL="--v=0"

# Should this cluster be allowed to run privileged docker containers
KUBE_ALLOW_PRIV="--allow-privileged=false"

# How the controller-manager, scheduler, and proxy find the apiserver
#KUBE_MASTER="--master=http://127.0.0.1:8080"
KUBE_MASTER="--master=http://vm2:8080"
KUBE_ETCD_SERVERS="--etcd_servers=http://vm2:4001"

```

```shell
 [root@vm2 kubernetes]# vim apiserver
# kubernetes system config
#
# The following values are used to configure the kube-apiserver
#

# The address on the local server to listen to.
KUBE_API_ADDRESS="--insecure-bind-address=0.0.0.0"

# The port on the local server to listen on.
KUBE_API_PORT="--port=8080"

# Port minions listen on
# KUBELET_PORT="--kubelet-port=10250"

# Comma separated list of nodes in the etcd cluster
KUBE_ETCD_SERVERS="--etcd-servers=http://vm2:2379"

# Address range to use for services
KUBE_SERVICE_ADDRESSES="--service-cluster-ip-range=10.254.0.0/16"

# default admission control policies
# KUBE_ADMISSION_CONTROL="--admission-control=NamespaceLifecycle,NamespaceExists,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota" 去掉ServiceAccount,否则需要认证
KUBE_ADMISSION_CONTROL="--admission-control=NamespaceLifecycle,NamespaceExists,LimitRanger,SecurityContextDeny,ResourceQuota"

# Add your own!
KUBE_API_ARGS=""
```

```shell
systemctl restart kube-apiserver	systemctl enable kube-apiserver
systemctl restart kube-controller-manager	systemctl enable kube-controller-manager
systemctl restart kube-scheduler	systemctl enable kube-scheduler
ps -ef | grep kube
```

5.2Node 节点（2个）

```shell
yum -y install kubernetes
# yum安装过程如果中出现下面问题
docker-ce-cli conflicts with 2:docker-1.13.1-103.git7f2769b.el7.centos.x86_64
docker-ce conflicts with 2:docker-1.13.1-103.git7f2769b.el7.centos.x86_64
# 之前安装过docker，先卸载，再重新安装kubernetes
yum list installed | grep docker
yum -y remove  containerd.io.x86_64  docker-ce.x86_64  docker-ce-cli.x86_64
yum -y install kubernetes
```

```shell
cd /etc/kubernetes
```

```shell
[root@vm1 kubernetes]# vim config
# kubernetes system config
#
# The following values are used to configure various aspects of all
# kubernetes services, including
#
#   kube-apiserver.service
#   kube-controller-manager.service
#   kube-scheduler.service
#   kubelet.service
#   kube-proxy.service
# logging to stderr means we get it in the systemd journal
KUBE_LOGTOSTDERR="--logtostderr=true"

# journal message level, 0 is debug
KUBE_LOG_LEVEL="--v=0"

# Should this cluster be allowed to run privileged docker containers
KUBE_ALLOW_PRIV="--allow-privileged=false"

# How the controller-manager, scheduler, and proxy find the apiserver
KUBE_MASTER="--master=http://vm2:8080"
KUBE_ETCD_SERVERS="--etcd_servers=http://vm2:4001"
```

```shell
[root@vm3 kubernetes]# vim config
# kubernetes system config
#
# The following values are used to configure various aspects of all
# kubernetes services, including
#
#   kube-apiserver.service
#   kube-controller-manager.service
#   kube-scheduler.service
#   kubelet.service
#   kube-proxy.service
# logging to stderr means we get it in the systemd journal
KUBE_LOGTOSTDERR="--logtostderr=true"

# journal message level, 0 is debug
KUBE_LOG_LEVEL="--v=0"

# Should this cluster be allowed to run privileged docker containers
KUBE_ALLOW_PRIV="--allow-privileged=false"

# How the controller-manager, scheduler, and proxy find the apiserver
KUBE_MASTER="--master=http://vm2:8080"
KUBE_ETCD_SERVERS="--etcd_servers=http://vm2:4001"
```

```shell
[root@vm1 kubernetes]# vim kubelet
# kubernetes kubelet (minion) config

# The address for the info server to serve on (set to 0.0.0.0 or "" for all interfaces)
KUBELET_ADDRESS="--address=0.0.0.0"

# The port for the info server to serve on
# KUBELET_PORT="--port=10250"

# You may leave this blank to use the actual hostname
KUBELET_HOSTNAME="--hostname-override=vm1"

# location of the api-server
KUBELET_API_SERVER="--api-servers=http://vm2:8080"

# pod infrastructure container
KUBELET_POD_INFRA_CONTAINER="--pod-infra-container-image=registry.access.redhat.com/rhel7/pod-infrastructure:latest"

# Add your own!
KUBELET_ARGS=""
```

```shell
[root@vm3 kubernetes]# vim kubelet
# kubernetes kubelet (minion) config

# The address for the info server to serve on (set to 0.0.0.0 or "" for all interfaces)
KUBELET_ADDRESS="--address=0.0.0.0"

# The port for the info server to serve on
# KUBELET_PORT="--port=10250"

# You may leave this blank to use the actual hostname
KUBELET_HOSTNAME="--hostname-override=vm3"

# location of the api-server
KUBELET_API_SERVER="--api-servers=http://vm2:8080"

# pod infrastructure container
KUBELET_POD_INFRA_CONTAINER="--pod-infra-container-image=registry.access.redhat.com/rhel7/pod-infrastructure:latest"

# Add your own!
KUBELET_ARGS=""
```

```shell
systemctl restart kube-proxy	systemctl enable kube-proxy
systemctl restart kubelet	systemctl enable kubelet
 ps -ef | grep kube
```

5.3查看节点信息描述

```shell
[root@vm2 ~]# kubectl describe node vm1
Name:			vm1
Role:			
Labels:			beta.kubernetes.io/arch=amd64
			beta.kubernetes.io/os=linux
			kubernetes.io/hostname=vm1
Taints:			<none>
CreationTimestamp:	Fri, 13 Dec 2019 17:42:24 +0800
Phase:			
Conditions:
  Type			Status	LastHeartbeatTime			LastTransitionTime			Reason				Message
  ----			------	-----------------			------------------			------				-------
  OutOfDisk 		False 	Sat, 14 Dec 2019 11:03:24 +0800 	Sat, 14 Dec 2019 09:52:52 +0800 	KubeletHasSufficientDisk 	kubelet has sufficient disk space available
  MemoryPressure 	False 	Sat, 14 Dec 2019 11:03:24 +0800 	Fri, 13 Dec 2019 17:42:26 +0800 	KubeletHasSufficientMemory 	kubelet has sufficient memory available
  DiskPressure 		False 	Sat, 14 Dec 2019 11:03:24 +0800 	Fri, 13 Dec 2019 17:42:26 +0800 	KubeletHasNoDiskPressure 	kubelet has no disk pressure
  Ready 		True 	Sat, 14 Dec 2019 11:03:24 +0800 	Sat, 14 Dec 2019 09:52:52 +0800 	KubeletReady 			kubelet is posting ready status
Addresses:		192.168.168.128,192.168.168.128,vm1
Capacity:
 alpha.kubernetes.io/nvidia-gpu:	0
 cpu:					1
 memory:				1863076Ki
 pods:					110
Allocatable:
 alpha.kubernetes.io/nvidia-gpu:	0
 cpu:					1
 memory:				1863076Ki
 pods:					110
System Info:
 Machine ID:			35cadfa0752d49be8d21c0415a60bf9c
 System UUID:			BE584D56-C23D-3655-EC48-00B0BBD00E00
 Boot ID:			2363e4ea-9206-4ba9-adc3-016f7d6acbbc
 Kernel Version:		3.10.0-1062.el7.x86_64
 OS Image:			CentOS Linux 7 (Core)
 Operating System:		linux
 Architecture:			amd64
 Container Runtime Version:	docker://1.13.1
 Kubelet Version:		v1.5.2
 Kube-Proxy Version:		v1.5.2
ExternalID:			vm1
Non-terminated Pods:		(0 in total)
  Namespace			Name		CPU Requests	CPU Limits	Memory Requests	Memory Limits
  ---------			----		------------	----------	---------------	-------------
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.
  CPU Requests	CPU Limits	Memory Requests	Memory Limits
  ------------	----------	---------------	-------------
  0 (0%)	0 (0%)		0 (0%)		0 (0%)
Events:
  FirstSeen	LastSeen	Count	From			SubObjectPath	Type		Reason			Message
  ---------	--------	-----	----			-------------	--------	------			-------
  43m		43m		1	{kubelet vm1}				Normal		Starting		Starting kubelet.
  43m		43m		1	{kubelet vm1}				Warning		ImageGCFailed		unable to find data for container /
  43m		43m		1	{kubelet vm1}				Normal		NodeHasSufficientDisk	Node vm1 status is now: NodeHasSufficientDisk
  43m		43m		1	{kubelet vm1}				Normal		NodeHasSufficientMemory	Node vm1 status is now: NodeHasSufficientMemory
  43m		43m		1	{kubelet vm1}				Normal		NodeHasNoDiskPressure	Node vm1 status is now: NodeHasNoDiskPressure
  43m		43m		1	{kubelet vm1}				Normal		Starting		Starting kubelet.
  43m		43m		1	{kubelet vm1}				Warning		ImageGCFailed		unable to find data for container /
  43m		43m		1	{kubelet vm1}				Normal		NodeHasSufficientDisk	Node vm1 status is now: NodeHasSufficientDisk
  43m		43m		1	{kubelet vm1}				Normal		NodeHasSufficientMemory	Node vm1 status is now: NodeHasSufficientMemory
  43m		43m		1	{kubelet vm1}				Normal		NodeHasNoDiskPressure	Node vm1 status is now: NodeHasNoDiskPressure
  42m		42m		1	{kube-proxy vm1}			Normal		Starting		Starting kube-proxy.
```

5.4创建覆盖网络

在master和node节点均执行如下命令

```shell
yum -y install flannel
```

```shell
[root@vm2 ~]# vim /etc/sysconfig/flanneld
# Flanneld configuration options  

# etcd url location.  Point this to the server where etcd runs
FLANNEL_ETCD_ENDPOINTS="http://vm2:2379"

# etcd config key.  This is the configuration key that flannel queries
# For address range assignment
FLANNEL_ETCD_PREFIX="/atomic.io/network"

# Any additional options that you want to pass
#FLANNEL_OPTIONS=""
```

在master节点执行

```shell
# 配置etcd中关于flannel的key
[root@vm2 ~]# etcdctl mk /atomic.io/network/config '{ "Network": "10.0.0.0/16" }'
{ "Network": "10.0.0.0/16" }
```

重启服务

```shell
# master上执行
systemctl restart flanneld	systemctl enable flanneld
systemctl restart docker	systemctl enable docker 
systemctl restart kube-apiserver	systemctl enable kube-apiserver
systemctl restart kube-controller-manager	systemctl enable kube-controller-manager
systemctl restart kube-scheduler	systemctl enable kube-scheduler	
```

```shell
# node上执行
systemctl restart flanneld	systemctl enable flanneld
systemctl restart docker	systemctl enable flanneld
systemctl restart kubelet	systemctl enable kubelet
systemctl restart kube-proxy	systemctl enable kube-proxy
```

5.5查看集群节点

```shell
[root@vm2 ~]# kubectl get nodes
NAME      STATUS    AGE
vm1       Ready     17h
vm3       Ready     17h
```